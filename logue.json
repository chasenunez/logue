{
  "entries": [
    {
      "timestamp": "2025_09_05_14_47_13",
      "text": "This i sthsu",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_14_47_26",
      "text": "Ths is a trial to see if this is actually working.",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_14_48_14",
      "text": "Ok so there is actually a lot going on here - the first is that i think it is working. I can see what I am typing in the json file, ad I believe it is also updating in github",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_14_49_26",
      "text": "I think the only thing we should do next is make is such that I can actually see the things I am  typibg. I wonder if that is actually going to be super hard?",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_14_50_05",
      "text": "I wonder what would happen if i used a #hashtag or #tagging more generally. will it #show up as a separate #tag?",
      "tags": [
        "hashtag",
        "tagging",
        "show",
        "tag"
      ]
    },
    {
      "timestamp": "2025_09_05_15_04_43",
      "text": "Ok so now that we have made the edits to the code I can see what I am typing, and also i can see what has been typed before with smaller notation of the time. the problem is that now i am writing over",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_06_08",
      "text": "on the map",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_06_29",
      "text": "whats going on with the times too?",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_06_50",
      "text": "are time times random?",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_07_01",
      "text": "yeah a bit",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_07_09",
      "text": "g",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_07_17",
      "text": "g",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_07_23",
      "text": "g",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_27_36",
      "text": "this is a new low and I will make it a long one to see how the new spacing provided will work. I think that aside from the borders moving, it appears to work ok. but now comes the big problem, i think I m abouyt to write over the old logs, which i think we really dont want to do. I wonder if instead it will be better to do this side by side?",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_27_58",
      "text": "ƚ",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_28_05",
      "text": "ok so how about now?",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_29_19",
      "text": "ok wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_38_53",
      "text": "Another entry to show the differences\\",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_49_29",
      "text": "ok so here is a new entry",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_50_00",
      "text": "here is a new entry.",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_50_10",
      "text": "and i think there is a problem",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_50_18",
      "text": "how about now",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_50_37",
      "text": "how about now",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_50_44",
      "text": "i think not",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_54_48",
      "text": "this was almost perfect",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_54_55",
      "text": "ƚ",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_15_55_29",
      "text": "i think this one only really needs help with the whole writing over the rest thing",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_03_26",
      "text": "ddfdsfsdfsdlf",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_03_45",
      "text": "how about if I resize the window",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_04_20",
      "text": "how about now",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_05_30",
      "text": "ok we are back to this little gal",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_05_47",
      "text": "and honestly it is probably just fiine",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_08_11",
      "text": "so what happens now",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_08_37",
      "text": "beautiful. She is jankey, but i think very functional and I am happy about that",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_10_44",
      "text": "Ok so now I am making a log of short things i am doing and i think it will work for that",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_11_24",
      "text": "however, when I get longer aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa it overwrite the display of the old logs",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_26_38",
      "text": "this is an update",
      "tags": []
    },
    {
      "timestamp": "2025_09_05_16_55_23",
      "text": "ok this now works",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_08_06_19",
      "text": "returning emails and messages sent over the weekend, of particular note is Open Research Information Panel Discussion hosted by the University of Zurich,",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_08_06_51",
      "text": "as part of the project NAIF â National Approach for Interoperable Repositories and Findable Research Results (funded by swissuniversities).",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_08_10_28",
      "text": "Some updates to #stardate that i can do later this week: the ability to move and edit text with the arrowkeys. currently its sort of a one and done situation, which isn't bad, but it also isnt good. otherwise the user interface is functional, of not refined.",
      "tags": [
        "stardate"
      ]
    },
    {
      "timestamp": "2025_09_08_08_11_13",
      "text": "#stardate could also re-label the wondow it os operating in as \"stardate\"",
      "tags": [
        "stardate"
      ]
    },
    {
      "timestamp": "2025_09_08_09_43_06",
      "text": "lets give this a try this time. def load_logs() -> List[Dict[str, Any]]:",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_07",
      "text": "if not LOGFILE.exists():",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_07",
      "text": "return []",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_08",
      "text": "try:",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_09",
      "text": "with LOGFILE.open(\"r\", encoding=\"utf-8\") as f:",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_09",
      "text": "return json.load(f)",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_10",
      "text": "except (json.JSONDecodeError, OSError) as e:",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_11",
      "text": "print(f\"Error reading log file: {e}\", file=sys.stderr)",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_12",
      "text": "return []",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_29",
      "text": "how about htis",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_43_54",
      "text": "we are back, i guess to no longer being able to see what is is that I have typed",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_53_02",
      "text": "Ok so this is the first update since the code clean up and everything seems to be working really well.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_53_51",
      "text": "the scrolling window is a really smart idea, and i think the text wrapping was always the cause of my problems. Next I'd like to see if i can paste some carriage returns to see if that breaks everything",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_53_57",
      "text": "Kept the window title set to stardate.\"",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_53_58",
      "text": "[main b6b8a94] Input editor is now single-line with horizontal scrolling (get_singleline_input).",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_53_58",
      "text": "1 file changed, 99 insertions(+), 57 deletions(-)",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_53_59",
      "text": "nunezcha@bib-nunezcha-m stardate % git push",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_00",
      "text": "Enumerating objects: 5, done.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_00",
      "text": "Counting objects: 100% (5/5), done.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_01",
      "text": "Delta compression using up to 10 threads",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_02",
      "text": "Compressing objects: 100% (3/3), done.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_02",
      "text": "Writing objects: 100% (3/3), 2.06 KiB | 2.06 MiB/s, done.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_03",
      "text": "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_04",
      "text": "To https://gitlab.eawag.ch/chase.nunez/stardate.git",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_04",
      "text": "f54dd08..b6b8a94  main -> main",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_54_41",
      "text": "ok so thats a big no. I think we are doing single lines only. but perhaps that is in the best interest of the log in any case",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_58_49",
      "text": "the next entry will have all previous log entries logged using tui-journal.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_09_59_08",
      "text": "[{\"id\":0,\"date\":\"2025-08-22T00:00:00Z\",\"title\":\"2025_08_22_Friday\",\"content\":\"[1616] after much trial and error, I think I hav finally figured out how to get a digital log up and running. \\nwhat was once hand written is now digital and searchable and version controlled\\nor will be version controlled once I am able to upload to gitlab. \\ngoing forward each of these journals will be a log of a different day. \\neach entry within the day will have a time stamp with the local time (24 hours clock, offset by square brackets on either side). \\nthe point of the log will be to create a log of how I am spending my days here at Lib4RI. \\nI am here on a limited contract. I hope to have it made permenant. But first i must show that I am able to do good work here, and that involves perhaps a bit of tracking. \\n\\n[1619] i am beginning this log somewhat early in my tenure, but also somewhat late - it is the end of my third week.\\nongoing projects are manifold, but in general look like this:\\n\\nthe wiki for research data management is here:\\nhttps://www.wiki.lib4ri.ch/display/DM/Research+Data+Management\\n\\nthe projects page looks like this:\\n\\nProject name\\nProposal\\nLib Lead\\nstatus\\ntime spent (hr)\\nDatalakes   \\nEK  in progress 50\\nSandec  \\nEK  in progress 1\\nOntologies  \\nDMPs    \\nFM & CN  in progress 25\\nRenku   \\nOpenBIS \\nlib website \\nCN  in progress 10\\nEnviDat QA/QC   \\nCN  in progress 60\\nAI for RDM  \\nMUN in progress \\nSciCat  \\nCN  in progress 10\\nRDM survey  \\nEK  in progress 4\\nData availability statements    \\nEK & JB in progress 1\\nZenodo Guidance \\nCN  in progress 5\\nLicensing_Guidance  \\nCN  in progress 8\\n\\nI am most seriously working on QA/QC frictionless code for EnviDat at WSL. \\n\\nThis will be a public log, and likely read by superiors and others, so will not contain any passwords, gossip, or other juicy secrets. \\nsee you on monday where I will work with FM more on DMP's. And also some work on the qaqc. \\n\\ngot it to work on my github, so version control, here we come.\",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":1,\"date\":\"2025-08-25T00:00:00Z\",\"title\":\"2025_08_25_Monday\",\"content\":\"meeting with folks online for knowledge exchange at 10\\nmeeting with infoservices and document delivery at 13\\n?seeing apartment at 1700.\\n\\n[0755] at work answering emails\\n[0802] opening up the envidat_frictioness gitlab project https://gitlab.eawag.ch/chase.nunez/envidat_frictionless.git\\n### 1. **Introduction to the Frictionless Framework**\\n\\nThe **[Frictionless Framework](https://framework.frictionlessdata.io/)** offers [open-source tools](https://github.com/frictionlessdata) that help manage and ensure data quality. Its ideal for use cases like **[EnviDat](https://www.envidat.ch/#/)**, where the integrity and reliability of data are critical for [long term storage and reuse](https://www.dora.lib4ri.ch/wsl/islandora/object/wsl:18703). It has been used successfully in similar repositories like [DRYAD](https://blog.datadryad.org/2020/11/18/frictionless-data/) and the [Global Biodiversity Information Facility (GBIF)](https://data-blog.gbif.org/post/frictionless-data-and-darwin-core/) for data validation and quality control.\\n\\n### 2. **Big Picture: Why Frictionless for EnviDat?**\\n\\nFor **EnviDat**, quality assurance (QA) and control (QC) of uploaded ecological datasets is essential. Frictionless offers:\\n\\n* **Flexibility**: Researchers can validate their own datasets using a graphical interface (**[Open Data Editor](https://okfn.org/en/projects/open-data-editor/)**) while SciIT staff can run backend checks through Python scripts.\\n* **Automates the Validation Process**: Ensures all incoming data fits the expected structure, types, and constraints prior to ingestion while freeing up staff for more complex tasks. \\n* **Catches Common as well as Specific Errors**: Missing values, incorrect types, or malformed columns are standard, but common data error in ecological data can be added via a custom schema. \\n\\n### 3. **Example Dataset with Errors**\\n\\nLets take a look at a **subsample of a real biomass dataset** uploaded to EnviDat, but we will introduce some intentional errors (missing values, blank headers, blank rows, NA's, data type error, etc.).\\n\\nOriginal data (with no errors) come from _\\\"Herbivory mediates the response of below-ground food-webs to invasive grasses\\\"_, published in the _Journal of Animal Ecology_.\\n> _Fioratti, M., Cordero, I., Chinn, N., Firn, J., Holmes, J., Klein, M., Lebbink, G., Nielsen, U., Schtz, M., Zimmermann, S., Risch, A. C. (2025). Herbivory mediates the response of below-ground food-webs to invasive grasses. EnviDat. https://www.doi.org/10.16904/envidat.677._\\n\\n```python\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom io import StringIO\\n\\ncsv_text = \\\"\\\"\\\"\\nSite.ID,Biomasstype,Site,Invasion,Treatment,Weight_20by100_cm,sample_type\\n1,Litter,PnK,Native,Open,15.515,\\n1,Living,PnK,Native,Open,95.89,\\n2,Litter,PnK,Native,No livestock,39.14,\\n2,Living,PnK,,No livestock,177.355,\\n3,Litter,PnK,Native,No mammals,38.95,\\nerror,Living,PnK,Native,No mammals,117.16,\\n,,,,,,\\n,,,,,,red\\n9,Litter,Vivan,Native,Open,NA,\\n9,Living,Vivan,Native,Open,86.74,\\n10,Litter,Vivan,Native,No livestock,79.08,\\n10,Living,Vivan,Native,No livestock,110.51,\\n11,Litter,Vivan,Native,No mammals,85.83,\\n11,Living,Vivan,Native,No mammals,114.195,\\n\\\"\\\"\\\".strip()+\\\"\\\\n\\\"\\n\\nPath(\\\"biomass_sample.csv\\\").write_text(csv_text, encoding=\\\"utf-8\\\")\\nprint(\\\"Wrote biomass_sample.csv\\\")\\n\\n\\n# Load the dataset into pandas DataFrame\\ndf = pd.read_csv(StringIO(csv_text))\\ndf.head()  # Displaying the first few rows of the dataset\\n```\\n### 4. **Step-by-Step: How Frictionless Helps QC and QA**\\n\\n#### 4.1 **Using the Open Data Editor (GUI) for QA**:\\n\\nResearchers can use the **[Open Data Editor](https://okfn.org/en/projects/open-data-editor/)** to interactively validate their datasets before uploading them to EnviDat. The Open Data Editor provides a graphical interface that checks data quality in real-time.\\n\\n* **What the Open Data Editor Can Catch**:\\n\\n  * **Missing values** (e.g., \\\"NA\\\" entries in `Weight_20by100_cm` column).\\n  * **Incorrect data types** (e.g., text in numeric fields).\\n  * **Empty or missing columns**.\\n  * **Outliers** (values that fall outside the expected range).\\n\\n#### Example: Fixing Missing Data in the Open Data Editor (GUI)\\n\\nResearchers can open their dataset, visualize errors, and directly modify them in the GUI. If a field has missing values, users can choose to replace them with the average or a specific value.\\n\\n#### 4.2 **Using Frictionless in Python for Backend Validation (QC)**:\\n\\nOnce the preliminary check has been done by the researchers, Scientific IT staff can run a Python script (or work in the console) to validate datasets automatically before upload. This is done by defining a custom **schema** that specifies the expected structure and rules for the dataset.\\n\\n```python\\n# Install Frictionless Framework if necessary\\n!pip install frictionless\\n```\\n### 4.3 **What Errors Does the Default Schema Catch?**\\n\\n* **Missing or null values** (e.g., NA in numerical fields).\\n* **Type mismatches** (e.g., text in numeric columns).\\n* **Invalid values** (e.g., `Invasion` should be either \\\"Native\\\" or \\\"Invaded\\\").\\n* **Duplicate rows** or columns.\\n* **Empty or missing headers**.\\n\\n#### 4.4 **Custom Schema**:\\n\\nYou can **extend the default schema** to suit your own data structure. For example, you can set custom ranges for numeric columns, specific formats for strings, and define additional validation rules (e.g., a specific regex pattern for site names).\\n\\n```python\\nfrom frictionless import Schema, Resource, validate\\n\\nschema = Schema({\\n\\\"fields\\\": [\\n{\\\"name\\\": \\\"Site.ID\\\", \\\"type\\\": \\\"integer\\\", \\\"constraints\\\": {\\\"required\\\": True}},\\n{\\\"name\\\": \\\"Biomasstype\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": True, \\\"enum\\\": [\\\"Living\\\", \\\"Litter\\\"]}},\\n{\\\"name\\\": \\\"Site\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": True}},\\n{\\\"name\\\": \\\"Invasion\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": True, \\\"enum\\\": [\\\"Native\\\", \\\"Invaded\\\"]}},\\n{\\\"name\\\": \\\"Treatment\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": True, \\\"enum\\\": [\\\"Open\\\", \\\"No livestock\\\", \\\"No mammals\\\", \\\"No insects\\\"]}},\\n{\\\"name\\\": \\\"Weight_20by100_cm\\\", \\\"type\\\": \\\"number\\\", \\\"constraints\\\": {\\\"required\\\": True, \\\"minimum\\\": 0}},\\n{\\\"name\\\": \\\"sample_type\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": False}},\\n],\\n\\\"missingValues\\\": [\\\"\\\", \\\"NA\\\"]\\n})\\n\\nresource = Resource(path=\\\"biomass_sample.csv\\\", schema=schema)\\nreport = validate(resource)\\nprint(report.valid)\\n```\\n**Output Example**:\\n```json\\n{'valid': False,\\n 'stats': {'tasks': 1, 'errors': 10, 'warnings': 0, 'seconds': 0.025},\\n 'warnings': [],\\n 'errors': [],\\n 'tasks': [{'name': 'biomass_sample',\\n            'type': 'table',\\n            'valid': False,\\n            'place': 'biomass_sample.csv',\\n            'labels': ['Site.ID',\\n                       'Biomasstype',\\n                       'Site',\\n                       'Invasion',\\n                       'Treatment',\\n                       'Weight_20by100_cm',\\n                       'sample_type'],\\n            'stats': {'errors': 10,\\n                      'warnings': 0,\\n                      'seconds': 0.025,\\n                      'md5': 'db2b1002484257d4cd39d5d3dd642178',\\n                      'sha256': 'f3ed66bb58831f187b6d4da8468ac5647cff259f18667abfea9381122663a351',\\n                      'bytes': 549,\\n                      'fields': 7,\\n                      'rows': 14},\\n            'warnings': [],\\n            'errors': [{'type': 'constraint-error',\\n                        'title': 'Constraint Error',\\n                        'description': 'A field value does not conform to a '\\n                                       'constraint.',\\n                        'message': 'The cell \\\"\\\" in row at position \\\"5\\\" and '\\n                                   'field \\\"Invasion\\\" at position \\\"4\\\" does not '\\n                                   'conform to a constraint: constraint '\\n                                   '\\\"required\\\" is \\\"True\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'cells': ['2',\\n                                  'Living',\\n                                  'PnK',\\n                                  '',\\n                                  'No livestock',\\n                                  '177.355',\\n                                  ''],\\n                        'rowNumber': 5,\\n                        'cell': '',\\n                        'fieldName': 'Invasion',\\n                        'fieldNumber': 4},\\n                       {'type': 'type-error',\\n                        'title': 'Type Error',\\n                        'description': 'The value does not match the schema '\\n                                       'type and format for this field.',\\n                        'message': 'Type error in the cell \\\"error\\\" in row \\\"7\\\" '\\n                                   'and field \\\"Site.ID\\\" at position \\\"1\\\": type '\\n                                   'is \\\"integer/default\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'type is \\\"integer/default\\\"',\\n                        'cells': ['error',\\n                                  'Living',\\n                                  'PnK',\\n                                  'Native',\\n                                  'No mammals',\\n                                  '117.16',\\n                                  ''],\\n                        'rowNumber': 7,\\n                        'cell': 'error',\\n                        'fieldName': 'Site.ID',\\n                        'fieldNumber': 1},\\n                       {'type': 'blank-row',\\n                        'title': 'Blank Row',\\n                        'description': 'This row is empty. A row should '\\n                                       'contain at least one value.',\\n                        'message': 'Row at position \\\"8\\\" is completely blank',\\n                        'tags': ['#table', '#row'],\\n                        'note': '',\\n                        'cells': ['', '', '', '', '', '', ''],\\n                        'rowNumber': 8},\\n                       {'type': 'constraint-error',\\n                        'title': 'Constraint Error',\\n                        'description': 'A field value does not conform to a '\\n                                       'constraint.',\\n                        'message': 'The cell \\\"\\\" in row at position \\\"9\\\" and '\\n                                   'field \\\"Site.ID\\\" at position \\\"1\\\" does not '\\n                                   'conform to a constraint: constraint '\\n                                   '\\\"required\\\" is \\\"True\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'cells': ['', '', '', '', '', '', 'red'],\\n                        'rowNumber': 9,\\n                        'cell': '',\\n                        'fieldName': 'Site.ID',\\n                        'fieldNumber': 1},\\n                       {'type': 'constraint-error',\\n                        'title': 'Constraint Error',\\n                        'description': 'A field value does not conform to a '\\n                                       'constraint.',\\n                        'message': 'The cell \\\"\\\" in row at position \\\"9\\\" and '\\n                                   'field \\\"Biomasstype\\\" at position \\\"2\\\" does '\\n                                   'not conform to a constraint: constraint '\\n                                   '\\\"required\\\" is \\\"True\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'cells': ['', '', '', '', '', '', 'red'],\\n                        'rowNumber': 9,\\n                        'cell': '',\\n                        'fieldName': 'Biomasstype',\\n                        'fieldNumber': 2},\\n                       {'type': 'constraint-error',\\n                        'title': 'Constraint Error',\\n                        'description': 'A field value does not conform to a '\\n                                       'constraint.',\\n                        'message': 'The cell \\\"\\\" in row at position \\\"9\\\" and '\\n                                   'field \\\"Site\\\" at position \\\"3\\\" does not '\\n                                   'conform to a constraint: constraint '\\n                                   '\\\"required\\\" is \\\"True\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'cells': ['', '', '', '', '', '', 'red'],\\n                        'rowNumber': 9,\\n                        'cell': '',\\n                        'fieldName': 'Site',\\n                        'fieldNumber': 3},\\n                       {'type': 'constraint-error',\\n                        'title': 'Constraint Error',\\n                        'description': 'A field value does not conform to a '\\n                                       'constraint.',\\n                        'message': 'The cell \\\"\\\" in row at position \\\"9\\\" and '\\n                                   'field \\\"Invasion\\\" at position \\\"4\\\" does not '\\n                                   'conform to a constraint: constraint '\\n                                   '\\\"required\\\" is \\\"True\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'cells': ['', '', '', '', '', '', 'red'],\\n                        'rowNumber': 9,\\n                        'cell': '',\\n                        'fieldName': 'Invasion',\\n                        'fieldNumber': 4},\\n                       {'type': 'constraint-error',\\n                        'title': 'Constraint Error',\\n                        'description': 'A field value does not conform to a '\\n                                       'constraint.',\\n                        'message': 'The cell \\\"\\\" in row at position \\\"9\\\" and '\\n                                   'field \\\"Treatment\\\" at position \\\"5\\\" does not '\\n                                   'conform to a constraint: constraint '\\n                                   '\\\"required\\\" is \\\"True\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'cells': ['', '', '', '', '', '', 'red'],\\n                        'rowNumber': 9,\\n                        'cell': '',\\n                        'fieldName': 'Treatment',\\n                        'fieldNumber': 5},\\n                       {'type': 'constraint-error',\\n                        'title': 'Constraint Error',\\n                        'description': 'A field value does not conform to a '\\n                                       'constraint.',\\n                        'message': 'The cell \\\"\\\" in row at position \\\"9\\\" and '\\n                                   'field \\\"Weight_20by100_cm\\\" at position \\\"6\\\" '\\n                                   'does not conform to a constraint: '\\n                                   'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'cells': ['', '', '', '', '', '', 'red'],\\n                        'rowNumber': 9,\\n                        'cell': '',\\n                        'fieldName': 'Weight_20by100_cm',\\n                        'fieldNumber': 6},\\n                       {'type': 'constraint-error',\\n                        'title': 'Constraint Error',\\n                        'description': 'A field value does not conform to a '\\n                                       'constraint.',\\n                        'message': 'The cell \\\"NA\\\" in row at position \\\"10\\\" and '\\n                                   'field \\\"Weight_20by100_cm\\\" at position \\\"6\\\" '\\n                                   'does not conform to a constraint: '\\n                                   'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'tags': ['#table', '#row', '#cell'],\\n                        'note': 'constraint \\\"required\\\" is \\\"True\\\"',\\n                        'cells': ['9',\\n                                  'Litter',\\n                                  'Vivan',\\n                                  'Native',\\n                                  'Open',\\n                                  'NA',\\n                                  ''],\\n                        'rowNumber': 10,\\n                        'cell': 'NA',\\n                        'fieldName': 'Weight_20by100_cm',\\n                        'fieldNumber': 6}]}]}\\n```\\n### 5. **How to Integrate Frictionless into the EnviDat Workflow**\\n\\n#### 5.1 **Step 2: IT Staff Backend Validation**\\n\\n* **Automated Validation**: Once the dataset is uploaded, IT staff can run a backend Python script using Frictionless to validate the data before it's stored in EnviDat. This could use one official EnviDat schema, or could be a collection of schema depending on the data type. \\n\\n  * The script can be integrated into the EnviDat upload process.\\n\\n```python\\nfrom frictionless import Package\\n\\n# Create a validation package for uploaded dataset\\npackage = Package(resources=[{\\n    'name': 'ecological_data',\\n    'path': 'uploaded_data.csv',\\n    'schema': schema\\n}])\\n\\n# Validate the package\\npackage.validate()\\n\\n# If valid, allow upload; if not, flag for corrections\\nif package.valid:\\n    print(\\\"Data is valid, ready to upload.\\\")\\nelse:\\n    print(\\\"Data has errors:\\\", package.errors)\\n```\\nIf, after running the custom schema, there are still errors, the dataset can be sent back to the researchers for correction with the helpful output from frictionless. \\n\\n### 7. **Conclusion**\\n\\nBy implementing **Frictionless**, WSL can streamline data quality assurance and control for EnviDat, empowering both researchers and IT staff:\\n\\n* **Researchers**: Validate their datasets via the Open Data Editor, making corrections before upload.\\n* **IT Staff**: Automate backend validation with Python scripts to ensure that all incoming data complies with predefined quality standards.\\n\\n**Next Steps**:\\n\\n* Set up Frictionless in the EnviDat environment.\\n* Train researchers on how to use the Open Data Editor for self-checks.\\n* Automate backend validation using Frictionless Python scripts to ensure the integrity of all uploaded data.\\n\\n**Online Resources**:\\n\\n* https://framework.frictionlessdata.io/index.html\\n* https://colab.research.google.com/github/frictionlessdata/frictionless-py/blob/v4/site/docs/tutorials/notebooks/frictionless-RDM-workflows.ipynb#scrollTo=dc538394\\n\\n[0804] need to expand this read me a bit to include more examples. I will hand off to felix who can try the code and see if it works for him or not, then perhaps I will send to El to try next. \\n[0837] felix and janina catch up - setting up notebook. \\n[0857] meting with felix about envidat_frictionless and DMP templates. FM had a meeting with someone from WSL who said that their goal isnt exactly to have datachecls on the data, but rather to check the metadata. \\nThis is challenging because the metadata are housed in an iCSV(future) and in the NEAT format (present) and there are also a ton of research data that do not lend themselves to tabular checks. i.e shape files and images. \\nTherefore going forward, it will likely be a good idea to first pull a NEAT file, and an iCSV, and to see if the frictionless framework works for those files as well, or how it can be adjusted to either break the file into 1. metadata and 2. data for 2 separate checks. \\nfelix will go through the current gitlab project and see if it works, I will search for metadata files and learn about the  Extended Bakus-Naur form (EBNF) to uderstand the envidat teams metadata standards. \\n\\n[0903] the extended Backus-Naur form is a form of notation for context-free grammar. \\ncontains terminal symbols and non-terminal production rules which govern how terminal symbols can be combined into a valod sequence.\\nterminal symbols are alpha-numeric characters, punctuation marks, and whitespace characters. \\ne.g. \\ndigit excluding zero = \\\"1\\\" | \\\"2\\\" | \\\"3\\\" | \\\"4\\\" | \\\"5\\\" | \\\"6\\\" | \\\"7\\\" | \\\"8\\\" | \\\"9\\\" ;\\ndigit                = \\\"0\\\" | digit excluding zero ;\\n\\nThis production rule defines the nonterminal digit which is on the left side of the assignment. The vertical bar represents an alternative and the terminal symbols are enclosed with quotation marks followed by a semicolon as terminating character. Hence a digit is a 0 or a digit excluding zero that can be 1 or 2 or 3 and so forth until 9. \\ncan also look crazy like this:\\n\\ntwelve                          = \\\"1\\\", \\\"2\\\" ;\\ntwo hundred one                 = \\\"2\\\", \\\"0\\\", \\\"1\\\" ;\\nthree hundred twelve            = \\\"3\\\", twelve ;\\ntwelve thousand two hundred one = twelve, two hundred one ;\\n\\nAn option can be represented through squared brackets [ ... ]. That is, everything that is set within the square brackets may be present just once, or not at all:\\n\\ninteger = \\\"0\\\" | [ \\\"-\\\" ], positive integer ;\\nTherefore, an integer is a zero (0) or a positive integer that may be preceded by an optional minus sign.\\n\\n[0911] i think now that that is out of the way, I am going to try and collect an iCSV example (SNOWPAT: https://code.wsl.ch/patrick.leibersperger/snowpat) and a NEAD example (https://github.com/GEUS-Glaciology-and-Climate/NEAD?tab=readme-ov-file)\\n[0940] pulling togeather examples from the githubs, in a jupyter notebook i am calling \\\"envidat_icsv_nead_metadata.ipynb\\\"\\n[1001] m3 knowledge exchange for the t3, we have francesco varrato , julian dederke, fabian schmid and from 4RI we have myself, fabian felder, mushumi and felix\\nstill waiting for angela but we are introducing ourselves anyway\\n[1004] musuhumi listing out services: versoning with git, RDM services, some other\\nEPFL a variety of services\\nETHZ RDM services, plus a summer school, how does the audience differ? Whay do they offer all the different services?\\n- workshop is in spring and autumn, they cover research data life cycle, what is RDM and active research data management, and a workshop form the data sciences center. \\ntarget group early career scientists, and some others join from central services and faculty. summer school is scheduled, but during the year they can pick and choose when thery attend (Julian Dederke)\\nthey also offer 2 credits for the summer school. there is quote a log of overlap between courses. summer school on site. and during the year is in person. \\n\\n[1009] FABIAN SCHMIDT: flexability is important, a lot of lectures suring the courses as well (sciIT) \\nFF: asks about minimla module parts in order to get credit? doe they get a certificate? - yes, they can. but this is mostly marketing. \\nrare that people just pick one module - so they will usually picke more than one, even if they do not choose them all. \\nthree is a good guess, but they are confronted with a lot of no shows. \\n\\n[1016] EPFL 5 modules offered twice a year in english: 1.5 hours per module. credited courses: CANVAS, 3 days 1 credit; Open Science, 2 credits. Phd students for the credited courses. \\nlunch talks not considered training, but they are rather talks where a tool is introduced, and balances general and specific trainings. \\n1 hr, 4 VIP invited (professors, SDSC, etc) and they then they have a panel discussion, usually 60 people, \\ncredited courses are limited to 10 people.\\na specific DMP course, data storage solutions, ethics of data management, data and code publication. \\n\\nFF: which online? they do the genereic training online,and the rest are in person. \\n\\n[1023] ETHZ: was originally all in person, and now is all online. benefit for summer school, and there is a benefit because it is a whole week, and it increases participation. \\\\\\njulian says: after 202, the question was whether to go back to in person, and it was agreed that it was better to keep it 100 online, as long as it was ok with the instructors. \\n\\n[1026] EPFL did have arguments to go back to in person, more people online but not sure what they were doing becuase of black screens. difficult to gather information, so hard to see if the online participants were actually participating. \\nperhaps the geogrpahic size difference between ETHZ (bigger) meant a differet solution for EPFL (smaller). \\n\\n[1029] FF asks how they separate the tasks of training:\\n\\nEPFL: always share between 2 people, even if the course is only 1.5 hours. \\nused to have longer sessions, but 1.5 was the right balance between specificity and generality for most people\\n\\nETHZ: they have fixed rolls, and they sharing teaching to avoid over stressing.\\nchnage the order of who starts so that they can replace each other if someone is sick or needs to leave. \\nexercies take time, but ETHZ 2.5 - 3.5 hours\\n\\n[1035] MUN asks about marketing\\n\\nEPFL: faculty engage specific people, doctoral school and every faculty has an office, and there is a form that needs to be filled, and the course is described and then offered, and is therefore only once per year. \\nhas other trainings on-demand: a couple of times per year, labs will ask to come during a group meeting, and there is a presenation and then it becomes a conversation.\\nuseful for new professor, catches them as they are creating their labs. \\nDMP, or other requirements\\n2-3 times per year for a couple of hours maximum. \\n\\nETHZ: they dont have people asking for on demand training, but they have a form where people can fill in what they want. \\nstudents must have 12 credit points in extra research continuing education to graduate. \\nhowever, the departments vary in how this is implemented, and so there is a piecemeal application, and some departments require documentation, and some do not\\n\\n[1045] MUN asks about how to decide which topics to cover\\n\\nEPFL: not enough people were interested in some topics (DMP, e.g.) and so they shrink those parts for the credited courses. \\nmore specialized topics are smaller in general courses. (e.g. data proitection ethics)\\nthey have changed over time, there is no fixed material, but for general matrials, content can be retooled depnding who is signed up. \\n\\nETHZ: they havent chnaged the content since the program is in motion, they are doing ti 4 years.\\ntoo much content, so they have to choose what to lose. They want to keep the exercises, but sometimes eliminate the instruction.\\nfeedback from the courses seems to indicate that they like the courses, but alway want more workshop, less lecture.They also want more tangible examples. \\nmostly updating outdated course materialsthe courses/workshops have been extensively retooled in 2020, and 2022 they were restructured again, but since then they have remained mostly the same since then. \\nnew cloud services may need to be updated soon. \\nsummer school feedback was too good to have major changes. \\nredundancy of content (e.g. FAIR) introduced incrementally to accommodate people that come in for just one lesson. Can't completely control who was coming.\\n\\n[1056] FF: we are struggling with the same problems, targeting mainly phd students, we are balancing differences between resources available at each institute\\nMUN: asks for exchanges for once per year. \\nThere was an exchange last friday that we were not invited to, and that is a little akward. \\nevery three months. so perhaps we dont need to have an additonal meeting. \\nFF will approach mathias about maybe joinng these existing meetings, \\nJulian is reluctant to add more institutions. \\n\\n[1107] wrapping up the meeting. \\n\\n[1108] MUN says that based on this we can probably offer all online. \\nFF: ICTS requires that there are a minimum of they are in person. \\nCN: perhaps a flipped classroom could allow the course to be online, and the consultation happening afterwards, satisfying the requiremtns. \\nMUN: workshops are more time intensive to prepare and also administer\\nFF: lego exercies (1.5 hours, too long). \\nCN: perhaps stickers as a form of \\\"certification\\\"\\nFF: flipped classroom as the final test for the project 2026\\nMUN: FF should approach chiara and matias \\n2026 flipped classtroom pilot\\n\\n[1023] typing up meeting notes\\n\\n[1132] lunch\\n\\n[1205] meeting with felix and janina\\n\\n[1218] bug fixes with felix\\n\\nError 1: when running rhe line \\\"schema = Schema.from_descriptor('data/input/table-schema.json')\\\" in the example:\\n---------------------------------------------------------------------------\\n\\nFrictionlessException                     Traceback (most recent call last)\\n\\nCell In[8], line 5\\n\\n      2 from frictionless import Schema, fields, describe\\n\\n      4 #schema = describe('data/input/biomass_summary_errors.csv', type='schema') # from a resource path\\n\\n----> 5 schema = Schema.from_descriptor('data/input/table-schema.json') # from a descriptor path\\n\\n      6 #schema = Schema.from_descriptor({'fields': [{'name': 'id', 'type': 'integer'}]}) # from a descriptor\\n \\nFile ~\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\frictionless\\\\metadata\\\\metadata.py:183, in Metadata.from_descriptor(cls, descriptor, allow_invalid, **options)\\n\\n    181 if isinstance(class_type, str):\\n\\n    182     type = class_type\\n\\n--> 183 Class = cls.metadata_select_class(type)\\n\\n    184 Error = Class.metadata_Error or platform.frictionless_errors.MetadataError\\n\\n    185 Class.metadata_transform(descriptor)\\n \\nFile ~\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\frictionless\\\\metadata\\\\metadata.py:293, in Metadata.metadata_select_class(cls, type)\\n\\n    291     note = f'unsupported type for \\\"{cls.metadata_type}\\\": {type}'\\n\\n    292     Error = cls.metadata_Error or platform.frictionless_errors.MetadataError\\n\\n--> 293     raise FrictionlessException(Error(note=note))\\n\\n    294 return cls\\n \\nFrictionlessException: [schema-error] Schema is not valid: unsupported type for \\\"schema\\\": ['string', 'object']\\n \\nError 2 when running \\\"package = Package(resources=[{\\n    'name': 'ecological_data',\\n    'path': 'uploaded_data.csv',\\n    'schema': schema\\n}])\\\" from the readme file\\n---------------------------------------------------------------------------\\n\\nAttributeError                            Traceback (most recent call last)\\n\\nCell In[5], line 4\\n\\n      1 from frictionless import Package\\n\\n      3 # Create a validation package for uploaded dataset\\n\\n----> 4 package = Package(resources=[{\\n\\n      5     'name': 'ecological_data',\\n\\n      6     'path': 'uploaded_data.csv',\\n\\n      7     'schema': schema\\n\\n      8 }])\\n \\nFile ~\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\frictionless\\\\package\\\\factory.py:44, in Factory.__call__(cls, source, control, basepath, packagify, *params, **options)\\n\\n     36     return cast(\\n\\n     37         platform.frictionless.Package,\\n\\n     38         cls.from_descriptor(source, basepath=basepath, **options),  # type: ignore\\n\\n     39     )\\n\\n     41 # Default\\n\\n     42 return cast(\\n\\n     43     platform.frictionless.Package,\\n\\n---> 44     type.__call__(cls, basepath=basepath, **options),\\n\\n     45 )\\n \\nFile <attrs generated methods frictionless.package.package.Package>:60, in __init__(self, source, control, basepath, name, title, description, homepage, profile, licenses, sources, contributors, keywords, image, version, created, resources, dataset, dialect, detector)\\n\\n     58 self._dialect = dialect\\n\\n     59 self._detector = detector\\n\\n---> 60 self.__attrs_post_init__()\\n \\nFile ~\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\frictionless\\\\package\\\\package.py:167, in Package.__attrs_post_init__(self)\\n\\n    165 def __attrs_post_init__(self):\\n\\n    166     for resource in self.resources:\\n\\n--> 167         resource.package = self\\n\\n    168         if self._dialect:\\n\\n    169             resource.dialect = self._dialect\\n \\nAttributeError: 'dict' object has no attribute 'package'\\n \\n[1300] mtg with infoservices and document delivery\\na small team spread thin over three locations and 4 research institutes\\nstepahnie is the group leader of the group, but bobby is also been here a lot. \\nrunning a info desk, in person and digital as well, \\non zoom and soon on teams\\nnot many zoom calls. \\nlibrary system has changed since 2020, swisscovery is the front end catalog, hold e resources and physical books\\nwe maintain the web layout of the catalog, and the relationship between the frontend and the back end. \\nAleph -> Alma (exlibris)\\nprimo -> swisscovery (exlibris) \\nin two years, Primo NDE (upgrade from primo VE)\\nintegration of the RI's into a national lab isnt a problem since they are already integrated. \\n500+ adacemic libraries in the system. \\n[1329] stephanie enters. talks a bit about the films for the library\\ndocument delivery - the acquistion of hard to retrieve resources. free for employees. \\na sort of interlibrary loan. \\nfebruary and march monitoring begins, and Stephanie looks to see if there is a journal that is frequently asked. \\n1300 requests per year, and they are usuallyu not from the same people\\nsocial science is usually the field that is mostly the resources requested. \\nDocument is free for all.\\n[1346] Acquisitons and cataloguing:\\nbuy less than 100 per year\\nprint media - and acquisitons\\n130 etc. , some monthly, some weekly, all come to PSI, are labeled and catalogued and then sent to the other RI's\\nDocuments and books afe often trashed when they are no longer able to be stored. \\nthe website used to be managed by infoservices, and now has been been re-assigned to the project managers. \\nInfo servies handles ssearch and report, and El and Janina have project managers \\nInfo services also creates and distributes the info sheets. \\nThey are updated frequently. \\nWe should make a 4 page info sheet on RDM resources/how to build a lab\\n[1407] trainings\\nleadtrainings are handling the trainings going forward, but for a while infoservices used to be in charge of the trainings\\ninfsoervies handle the reservations, as well as the logistics of booking rooms, preparation of the calendar, flyers, posters, and emails to reserachers, etc. \\nThere is a movement to make a harder requirement to gove credits to students, but thereire is a relucatance to make students do anything. \\ndeveloping tools for systematic literature searches \\ngroup stab statistics, in the internal resources\\nthe official library of statisticcs. \\n[1437] meeting ends\\n[1502] futzing with tjournal and its journal selection.\\nI would like to build on top of t journal some day to have it autimatically:\\ncreate a json file that is populated by a series of entries and version controlled automatically every hour. \\nbasically you would start the program in the terminal, and what you would see is a place to enter a note of some kind and hit enter.\\nom the back end, this entry would be given a time stamp, and would have metadata created for it so that entries that are logically grouped  by topic could be queried and searcherd later. \\nwhat you would end up with is a sort of personal twitter feed? where your entries will be stored and cataloged and version controlled withouth too much bother.\\nIf someone got in the habit of having the application window open, then you would have no barrier to making frequent updats to the log, and wouldnt have the bother of version controlling it or time stamping it. \\n[1533] meeting with felix, he is goingt o take on the future of the DMP, and I will focus on the envidat qc question. \\n[1546] now working again on getting the new icsv format from wsl so i can incorperate it into the envidata csv\\n[1610] FF stops by asking about music, meets with Demitrius, may move jour fixe meeting earlier tomorrow morning.\\n[1616] benchmarking my processing power with some silly tasks in r:\\n\\n# Start timer\\nstart_time <- Sys.time()\\n\\n# Set matrix size (can be increased for longer runs)\\nn <- 30000  # 10,000 x 10,000 matrix\\n\\n# Generate a large random matrix\\ncat(\\\"Generating matrix...\\\\n\\\")\\nA <- matrix(rnorm(n^2), nrow = n, ncol = n)\\n\\n# Matrix multiplication (A %*% A^T)\\ncat(\\\"Performing matrix multiplication...\\\\n\\\")\\nB <- A %*% t(A)\\n\\n# Eigenvalue decomposition\\ncat(\\\"Computing eigenvalues...\\\\n\\\")\\neig <- eigen(B, symmetric = TRUE, only.values = TRUE)\\n\\n# End timer\\nend_time <- Sys.time()\\nelapsed <- end_time - start_time\\n\\ncat(\\\"Total elapsed time:\\\", elapsed, \\\"\\\\n\\\")\\n\\n[1624] for a 30000x30000 matrix, the operation took Total elapsed time: 21.61286\\n[1647] updaing the tjournal formatting such that the dates, names, and tags are consistent. I think i will get rid of the \\\"lib4ri\\\" and \\\"data management\\\" tags in favor of a location tag (i.e. EAWAG, or PSI) since I think that will vary the most, and being able to see where I wrote something is valuable.\\nI am also thinking of having a standardize format of YYYY_MM_DD_Dayoftheweek. priority - 0, unless retroactively i think something is especially noteworthy for later, then it will be 1. therefore the only values will be [0,1].\\n\",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":2,\"date\":\"2025-08-26T00:00:00Z\",\"title\":\"2025_08_26_Tuesday\",\"content\":\"* jour fixe with FF at 9?\\n* sci-IT lunch with EMPA\\n* Apt. appt. at 1700h\\n\\n[0757] answering emails, hour fixe confirmed for 0900h. \\n\\n[0804] working again on envidat_frictionless project with iCSV project.\\n\\n#writing a iCSV\\nimport pysmet as icsv\\nimport pandas as pd\\n\\n# create a file object, with a filename, and a flag, to not read from a file\\nfile = icsv.iCSVFile() \\n\\n# create some example data, can of course also be read from CSV...\\ndata_pd = pd.DataFrame(data, columns=[\\\"T\\\", \\\"RH\\\", \\\"P\\\"])\\n\\n# will use the given column names as fields list\\nfile.setData(data_pd) \\n\\ndata_p = pd.DataFrame(data)\\n# will not work, as no column names are provided.\\nfile.setData(data_p) \\n# you can give a list of column names as well:\\nfile.setData(data_p, [\\\"T\\\",\\\"RH\\\",\\\"P\\\"])\\n\\n# set the metadata ( needs at least the required metadata, as a sanity check will be performed before writing)\\n# fields will already be set, if you followed the examples above\\nfile.metadata.set_attribute(\\\"field_delimiter\\\",\\\":\\\")\\nfile.metadata.set_attribute(\\\"geometry\\\",\\\"POINT(1 1)\\\")\\nfile.metadata.set_attribute(\\\"srid\\\",\\\"EPSG:1234\\\")\\n\\n\\nfile.info()\\nfile.write(out_filename)\\n\\n[0820] although this still doesnt really answer the main question, as I see it. Which is where is \\\"data\\\" coming from? i.e. \\npd.DataFrame(data, columns=[\\\"T\\\", \\\"RH\\\", \\\"P\\\"]) is still requiring an object \\\"data\\\" i think, which has not been specified in the code.\\n\\n[0831] i think i will \\nimport pysmet as icsv\\nimport pandas as pd\\n\\n# Sample data\\ndata = [\\n    [15.3, 60, 1013],  # T (Temperature), RH (Relative Humidity), P (Pressure)\\n    [14.8, 62, 1012],\\n    [16.1, 59, 1014]\\n]\\n\\n# create a file object\\nfile = icsv.iCSVFile()\\n\\n# Convert to pandas DataFrame\\ndata_pd = pd.DataFrame(data, columns=[\\\"T\\\", \\\"RH\\\", \\\"P\\\"])\\nfile.setData(data_pd)\\n\\n# Alternate versions\\ndata_p = pd.DataFrame(data)\\nfile.setData(data_p)  # This may fail due to missing column names\\nfile.setData(data_p, [\\\"T\\\", \\\"RH\\\", \\\"P\\\"])  # This should work\\n\\n# Set minimal metadata\\nfile.metadata.set_attribute(\\\"field_delimiter\\\", \\\":\\\")\\nfile.metadata.set_attribute(\\\"geometry\\\", \\\"POINT(1 1)\\\")\\nfile.metadata.set_attribute(\\\"srid\\\", \\\"EPSG:1234\\\")\\n\\nfile.info()\\nfile.write(\\\"output.icsv\\\")\\n\\n[0847] a few minor fixes to the code, and this is now working. the resulting \\\"icsv\\\" looks something like this:\\n\\nFile: None\\nMETADATA:\\nRequired:\\nfield_delimiter : :\\ngeometry : POINT(1 1)\\nsrid : EPSG:1234\\nRecommended:\\n\\nACDD Attributes:\\nUnknown Attributes:\\nfield_delimiter: :\\ngeometry: POINT(1 1)\\nsrid: EPSG:1234\\n\\nOther Metadata:\\n\\nFields: ['T', 'RH', 'P']\\nRecommended Fields:\\n\\nOther Fields:\\n\\nGeometry: None\\nSRID: None\\nLocation: X: None\\nY: None\\nZ: None\\nEPSG: None\\n\\nData:\\n      0   1     2\\n0  15.3  60  1013\\n1  14.8  62  1012\\n2  16.1  59  1014\\n\\n[0848] the thing which we have going for us is that the icsv and the NEAD format both have a clear breat between metadata and data\\ni.e. a \\\"data\\\" section. Now the question is:\\n1.  if we need to break them apart for QC or if they can be qc'd whole\\n2. if needed, how do we break and qc them. \\n3. how best to qc the metadata\\n4. can we use the qc's metadata to better qc the data?\\n\\n[0854] lvs to Jour Fixe with FF, EK, MUN in C71 starting at 0900\\n\\nMUN: MOOCathon updates: prizes and judges sorted\\nFF: OER call where once of the High Energy Physics MOOC catalog\\nEMPA offers a python course (CHF 600 for outsider, and free for EMPA emplpoyees)\\nFF: talked to the admin about moving a bit of money arounf to keep MUN employed by moving when the in kind contributions to next year. \\n\\nMUN: francesco talked about having a chatbot, would like to organize a meeting with him to discuss\\nalso want tos have a 4RI meeting\\n\\nFlorian Alternmatt going for an AI call, maybe FM would like to work with him.\\n\\nEK: setting up a meetingn with WSL\\nuni bern meeting incipient, but no response.\\n\\nFF: for datalakes: need to confirm how the json must look like for ingestion into ERIC\\nERIC and EnviDat likely wont merge anytime soon\\n\\nFF: asking LN about being included int he WSL consultations\\n\\n[0955] back at des working on the envidat qc problem\\n\\nneed to leave today by 1620 to catch the 1623 to make it to the appointment on time.\\n\\n[1012] updaing the window tiling software on my machine so i can easily have multiple tiled console windows open. \\n\\n[1045] moving back to the envidat/iCSV/NEAD data problem. \\n\\nMy hope is that we can use frictionless language in order to use the metadata in iCSV in order to QC the data by using the Metadata that preceeds the data to inform the schema\\nin this way the icsv has an important value add in that it allows the researchers to qc comment their data and qc it all with one file. \\n\\nthe question of course is how. In some ways is is just a formatting problem, but in other ways, its a bit of a pain to shuttle data between two standards.\\n\\n[1113] making a budget, pushing changes to envidat repo\\n\\n[1130] FF come in and reports that we are now allowed to attend the group meeting of the ETH and EPFL\\n\\n[1148] working with EK on the lib4RI RDM cards with mesh QR code. \\n\\n[1155] heading to the scientific IT lunch arranged by Stuart from EAWAG IT.\\n\\n[1315] back from work lunch, and dipping back into the envidat problem. starting with the newly created iCSV, and seeing if I can break it apart\\n\\n[1327] I need to write python code that:\\n\\n1. ingests an icsv\\n2. recongizes what parts are csv data and which parts are metadata\\n3. uses the infornation in the meta data to create a frictionless schema (and checks that the meta data are in order and comply with the metadata structural guidelines\\n4. uses that schema to use frictionless in order to check that the data comply with the metadata schema. \\n5. provides a human readable error report that tells if the metadata are complete and in order, (if not, telling what needs to change), as well as if the data match the metadata (and where to change if necessary)\\n\\ni already have code that uses an existing schema to quality check data using the frictionless framework (provided below)\\ni will also provide what an icsv looks like generally. \\n\\n[1416] this is going to be a bit of a heavy lift - coding wise. it may take a number of hours.\\n\\n> here is an example dataset that has introduced errors:\\n\\n```python\\nimport pandas as pd\\nfrom pathlib import Path\\nfrom io import StringIO\\n\\ncsv_text = \\\"\\\"\\\"\\nSite.ID,Biomasstype,Site,Invasion,Treatment,Weight_20by100_cm,sample_type\\n1,Litter,PnK,Native,Open,15.515,\\n1,Living,PnK,Native,Open,95.89,\\n2,Litter,PnK,Native,No livestock,39.14,\\n2,Living,PnK,,No livestock,177.355,\\n3,Litter,PnK,Native,No mammals,38.95,\\nerror,Living,PnK,Native,No mammals,117.16,\\n,,,,,,\\n,,,,,,red\\n9,Litter,Vivan,Native,Open,NA,\\n9,Living,Vivan,Native,Open,86.74,\\n10,Litter,Vivan,Native,No livestock,79.08,\\n10,Living,Vivan,Native,No livestock,110.51,\\n11,Litter,Vivan,Native,No mammals,85.83,\\n11,Living,Vivan,Native,No mammals,114.195,\\n\\\"\\\"\\\".strip()+\\\"\\\\n\\\"\\n\\nPath(\\\"biomass_sample.csv\\\").write_text(csv_text, encoding=\\\"utf-8\\\")\\nprint(\\\"Wrote biomass_sample.csv\\\")\\n\\n# Load the dataset into pandas DataFrame\\ndf = pd.read_csv(StringIO(csv_text))\\ndf.head()  # Displaying the first few rows of the dataset\\n\\n\\n```\\n> here is existing code that creates a schema to quality check data using the frictionless framework:\\n\\n```python\\nfrom frictionless import Schema, Resource, validate\\n\\nschema = Schema({\\n\\\"fields\\\": [\\n{\\\"name\\\": \\\"Site.ID\\\", \\\"type\\\": \\\"integer\\\", \\\"constraints\\\": {\\\"required\\\": True}},\\n{\\\"name\\\": \\\"Biomasstype\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": True, \\\"enum\\\": [\\\"Living\\\", \\\"Litter\\\"]}},\\n{\\\"name\\\": \\\"Site\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": True}},\\n{\\\"name\\\": \\\"Invasion\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": True, \\\"enum\\\": [\\\"Native\\\", \\\"Invaded\\\"]}},\\n{\\\"name\\\": \\\"Treatment\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": True, \\\"enum\\\": [\\\"Open\\\", \\\"No livestock\\\", \\\"No mammals\\\", \\\"No insects\\\"]}},\\n{\\\"name\\\": \\\"Weight_20by100_cm\\\", \\\"type\\\": \\\"number\\\", \\\"constraints\\\": {\\\"required\\\": True, \\\"minimum\\\": 0}},\\n{\\\"name\\\": \\\"sample_type\\\", \\\"type\\\": \\\"string\\\", \\\"constraints\\\": {\\\"required\\\": False}},\\n],\\n\\\"missingValues\\\": [\\\"\\\", \\\"NA\\\"]\\n})\\n\\nresource = Resource(path=\\\"biomass_sample.csv\\\", schema=schema)\\nreport = validate(resource)\\nprint(report.valid)\\n```\\n\\n> here is what an icsv looks like generally\\n\\n```python\\nfrom snowpat import pysmet as smet\\nfrom snowpat import snowpackreader as spr\\nfrom snowpat import icsv\\nimport pandas as pd\\n\\n# Sample data\\ndata = [\\n    [15.3, 60, 1013],  # T (Temperature), RH (Relative Humidity), P (Pressure)\\n    [14.8, 62, 1012],\\n    [16.1, 59, 1014]\\n]\\n\\n# create a file object\\nfile = icsv.iCSVFile()\\n\\n# Convert to pandas DataFrame\\ndata_pd = pd.DataFrame(data, columns=[\\\"T\\\", \\\"RH\\\", \\\"P\\\"])\\nfile.setData(data_pd)\\n\\n#set minimal metadata\\nfile.metadata.set_attribute[\\\"field_delimiter\\\", \\\":\\\"]\\nfile.metadata.set_attribute[\\\"geometry\\\", \\\"POINT\\\", \\\":\\\"]\\nfile.metadata.set_attribute[\\\"srid\\\", \\\"EPSG:1234\\\", \\\":\\\"]\\n\\nfile.info()\\nfile.write(\\\"output.icsv\\\")\\n```\\n> output looks like this:\\n\\nFile: None\\nMETADATA:\\nRequired:\\nfield_delimiter : :\\ngeometry : POINT(1 1)\\nsrid : EPSG:1234\\nRecommended:\\n\\nACDD Attributes:\\nUnknown Attributes:\\nfield_delimiter: :\\ngeometry: POINT(1 1)\\nsrid: EPSG:1234\\n\\nOther Metadata:\\n\\nFields: ['T', 'RH', 'P']\\nRecommended Fields:\\n\\nOther Fields:\\n\\nGeometry: None\\nSRID: None\\nLocation: X: None\\nY: None\\nZ: None\\nEPSG: None\\n\\nData:\\n      0   1     2\\n0  15.3  60  1013\\n1  14.8  62  1012\\n2  16.1  59  1014\\n[1448] working through the metadata extraction  \\n[1558] ok, feeling a bit brain-dead, switching to a renku integration question for wsl\\n[1603] accepting planned meeting with ionut and others at wsl for mid october\\n[1620] lvs for appt.\\n\",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":3,\"date\":\"2025-08-27T00:00:00Z\",\"title\":\"2025_08_27_Wednesday\",\"content\":\"*FF is on vacation\\n*need to go through the EnvidatQC code to see what works and what doesn't.\\n[0754] looking for keys that I think i left here last night, and I found them. \\n[0810] Federico shows up\\n[0827] the server holding the wiki has been updated, searching for irregularitie son the RDM section. \\n[0832] looking at a python course being offered by ETHz as a way of creating a \\\"RDM\\\" python course maybe.\\n[0858] updating the README.md in the TUI-Journal gitlab page, as well as uploading the zip file etc. for safekeeping. \\n[0909] now beginning work on the envidat icsv code. \\nTypeError                                 Traceback (most recent call last)\\nCell In[6], line 16\\n     13 file.setData(data_pd)\\n     15 #set minimal metadata\\n---> 16 file.metadata.set_attribute[\\\"field_delimiter\\\", \\\",\\\"]\\n     17 file.metadata.set_attribute[\\\"geometry\\\", \\\"POINT\\\", \\\":\\\"]\\n     18 file.metadata.set_attribute[\\\"srid\\\", \\\"EPSG:1234\\\", \\\":\\\"]\\nTypeError: 'method' object is not subscriptable\\n[0931] needed to change the square brackets to parentheses, and then get rid of the third argument in the second 2 calls. \\n[0932] now working with the problem of the icsv file read in.\\n[1004] learning about the structure of NetCDF's\\n[1015] editing EK's presentation on the RDM services\\n[1031] back at the iCSV project\\n[1054] bug fixes all the way down, but now we are are all clear to try the demo run.\\n[1122] Bobby comes in with concerns about some formatting problems in a document that they are writing with each other. Is concerned that there is no IT support. \\n[1145] headed to lunch with the library. \\n[1245] back from lunch and working on the python code for the envidat program\\n[1255] resolving a branch merge:\\n\\nnunezcha@bib-nunezcha-m tui-journal % git log --graph --decorate --oneline\\n* 14cc2de (HEAD -> main, origin/main, origin/HEAD) log updates\\n*   f5a4cab Merge branch 'main' of gitlab.eawag.ch:chase.nunez/captains_log\\n|\\\\\\n| * 8677f44 Add readme file\\n* | 0b6a4a8 log updates\\n|/\\n* 4a8f182 unzipping folder to see if this chnages things\\n* b3bfb1d adding the source code for tui journal as a zip file, not sure if this will bread everything or not - it's only 2 MB\\n* a662e7e log updates\\n* 8350665 log updates with more envidat ingestion metacode\\n* b1df14f log updates with more envidat ingestion metacode\\n* 741ef1b log updates with metacode for the new envidat module\\n* 8bff6f5 log updates\\n* 6144339 log updates\\n* 32b7159 adding data calls to the envidat icsv problem\\n* 405c987 adding chnages to tags, titles, and adding an entry for tomorrow\\n* e00977a adding chnages to tags, titles, and adding an entry for tomorrow\\n* 9a53271 adding chnages to tags, titles, and adding an entry for tomorrow\\n* 5e43fee adding chnages to tags, titles, and adding an entry for tomorrow\\n* 285127d log updates\\n* 5fc5467 log updates'\\n* 61bcba2 log updates\\n* a9b7da1 log updates\\n* 1ef26c4 captains log updates\\n* bf47bb8 log updates\\n* 45ae9b3 log entries update\\n* b201b09 adding log for monday already\\n* 6f231d5 adding a trial chnage in text to test version control\\n* 3638a20 a bit of a reconfiguration merging tui-journal and captains log\\n* 88b3f7d adding log_keeper.py, also changing log name\\n* dde739c initializing new project on library machine\\n* 240b4cc Configure Secret Detection in `.gitlab-ci.yml`, creating this file if it does not already exist\\n* 01d0f55 Initial commit\\n\\n[1305] I am calling the QAQC iCSV to frictionless schema \\\"FrictionShift\\\"\\n[1358] I am working through getting iTerm to update the profile presets. \\n[1420] futzing with default window operation on iterm\\n[2436] ok so the pytho package is running in console on the trial iCSV, but nothing is returned, so that could mean either it isn't functioning nomially, or if thre area no mistakes there are not reports. \\n[1437] I am immidiately suspiscious of code that works on the first time, so I will perhaps upload some real data as a iCSV, and see where the night takes us. \\n[1442] now going to try to use the code to transform the csv of the bimass data into an iCSV.\\n[1519] working on the RDM website, and specifically the research data pipeline using a .d3 sunburst.\\n[1608] I have fallen back into a d3.js hole and am creating a circular sunburst chart with the RDM services. \\n[1651] despite all odds, it is up and running and looks really good (i think). I will now create a project in gitlab and call it RDM_Sunburst\\n[1705] lvs\",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":4,\"date\":\"2025-08-28T00:00:00Z\",\"title\":\"2025_08_28_Thursday\",\"content\":\"**went to migrationamt for 0800 meeting for biometrie**\\n[0835] picking up where I left off with the sunburst graphic.\\n[0849] now that the sunburst is working in principle, i would like to add\\n    - a center graphic that either has an arrow or a label that says the reserach data lifecycle\\n    - add links to all services\\n    - identify services that we wish we offered\\n[0937] working in the code now to make the changes to the arrow\\n[1109] still working on this, currenlty trying to create two separate markets, to solve the problem of having the arrowhead blend in with the arrow body. I have been thinking about how to incorperate a white arrow behind that would offset it. this has proved more difficult than originally imagined. \\n[1138] lunch with the library\\n[1215] back working on the sunburst, and things are looking good! i have decided to make several elements transparent, and have introduced a few gradients, which have made it look a bit more polished.\\n[1311] updates: i have made a center gradient \\\"arrow\\\" signifying te research circle is in fact a cycle with directionality. I have also improved the aethetics of the cirlce and edited their actions dramatically.  Finally, i have chnaged the code to permit html links, as well as added the html links to the .json\\n[1431] I ahve now added a third layer for the RI specifics, i.e EAWAG, EMPA, PSI and WSL can havee different dmp templates.\\n[1449][main 68f59fe] updated sunburst code: 2 files changed, 39 insertions(+), 17 deletions(-)\\n[1451] now i am trying to complete the link tree using the resources here: https://www.lib4ri.ch/research-data-management-resources\\n[1657] it's fully linked out and now in the right order. I think next week we can try to get it on the website!\\n\\nPOSTHOC:\\nfrom the AUGUST timesheet:\\nTOTAL TIME = 187.6\\nOVERTIME = 19.10\\nHOLIDAY BALANCE 31.1\\nPER DAY: \\n10.17   10.17  09.17    10.17   10.17           \\n10.17   10.17  09.17    08.17   09.17            \\n09.42   09.17  09.42    09.42   09.17           \\n09.16   08.30  09.17    09.17  (08.50)\",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":5,\"date\":\"2025-09-01T00:00:00Z\",\"title\":\"2025_09_01_Monday\",\"content\":\"used my moving time allowance for last friday to clean and handover my apartment in Meersburg. \\n\\n[0754] emails and messages from last week\\n[0815] catching up on my rdm sunburst project, going to try to get it on stage today\\n[0822] sunburst working, cant get logged into stage\\n[0836] getting signed on to stage, but exceeded my allowance for sign ins\\n[0939] i currently have the graphic visible and hostyed on gitlab, but when I try to set it as a iframe, gitlab wont play ball. if i open the link in a private window, it askes for a log in. i need to fix this. \\n[1006] email sent to stuart in IT, now trying to use new PSI log in, but failing. \\n[1016] now that the navigation app is working in principle, and we are just waiting to get it on the website, I think that i am going to turn back to the envidat qc program. As I recall, I was able to get the frictionless framework working with a stndard csv. FF told me about their move to iCSV, so I began writing a program that injected that file type. \\n[1018] however, i think that things went a little off the rails when I attempted to use the metadata in the iCSV in order to create the schema that checks the data to make sure it fits with the expected ranges.\\n[1019] therefore, it think the task for me today is to 1) create an error filled dataset iCSV, then 2)run it through the program and 3) see if the returned report captures the errors correctly. \\n[1043] registerened for a talk about ELN's today at 14h on zoom: https://hu-berlin.zoom-x.de/w/69754819609?tk=3QWRpTYiJN--1rIyiRPZ-WNZ_Iy91yyhLFs8VlXfVTE.DQgAAAAQPbYUGRZHUnpIcU9CdFRpaWk1VVhuaFFYOW5BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA&pwd=QxncpP4CYbgiCoAiGRMRtpaBXaZXB2.1\\n[1145] going to lunch\\n[1234] meeting with FF: structure as a json, not as a markdown at firm (perhaps it could be visualzized earlier. -template answer based on question answers. \\n[1301] FF has resolved to create a branching structure app for the DMP tool that cerates a finished product that is based on user input. \\n[1321] [master 3f5e7cb] adding a new dataset:2 files changed, 33 insertions(+), 8 deletions(-)\\n[1400] for From Paper to Digital: The Human and Technical Challenges in Implementing Electronic Lab Notebooks for FAIR Data.\\n[1502] ELN talk was fine, but a bit remedial: mostly looking at the history of lab notebooks, and the challenges for implementing ELN's at scale. Mostly focusing on the challenegs of people power, funding, and \\\"one size fits all\\\" solutions.\\n[1504] working first on the envidat metadata checking. need to find what their metadata standards first.\\n[1512] for thursday, enhanceR conference in Freiborg, commute is 1h48min x 2. to calculate hours, I will essentially take off one hour to account for my usual commute time. that will mean that leaving at 7:19 - 19:19 = 12 hr, - 1hr, = 11hrs\\n[1545] made myself a cheat sheet for the EnhanceR conference on thursday:\\n\\n0719 - 0907 Commute zurich - Fribourg, Charmettes\\n\\n0930 -1000 registration coffee\\n\\n1000-1015 welcome\\n\\n1015 - 1100: keynote  Building Open-Source, Transparent, and Multilingual LLMs in Switzerland (Imanol Schlag)\\n\\n1100-1115: coffee break\\n\\nTRACK B ROOM 012\\n1115-1145 ST SESSION 1:\\nHow Rust enables you to create a domain specific language\\nJusong Yu (PSI)\\nOr \\nNTSuisse: a web platform for high-resolution mass spectrometry (HRMS) data\\nKai-Michael Kammer (Eawag)\\n\\n1145-1215 ST SESSION 2:\\nAFFORD: a workflow for data stewards to make the data FAIRer\\nGorka Fraga Gonzalez (UZH)\\n Or \\nBuilding Secure, GPU-Accelerated Applications on HPC Infrastructure\\nAhmad Alhineidi and Viktor Kovtun (UniBE)\\n\\n1215-1330: lunch\\n\\nTRACK A ROOM 001\\n1330 - 1415  WS SESSION 1:\\nConnecting code, data & compute for collaborative research with Renku\\nLaura Kinkead (SDSC)\\nor\\nUnderstand the value of Nix for stable development - The Fun Way\\nGabriel Nuetzi (SDSC)\\n\\n1415-1500 WS SESSION 2:\\nDeploy a FAIR Python application in 30 minutes using Gradio\\nSimon Duerr (HES-SO Valais-Wallis)\\nOr\\nRSEs and Data Stewards synergies finding\\nMoushumi Ulrich-Nath (Lib4RI)\\nOr\\nUnderstand the value of Nix for stable development - The Fun Way\\nGabriel Nuetzi (SDSC)\\n\\n15:30 - 1615: KEYNOTE\\nScaling Jupyter to millions of users (Sylvain Corlay)\\n\\nTRACK A ROOM 001\\n1615-1645: ST SESSION 3\\nPoC or Prod: What makes AI projects successful?\\nRoman Wixinger and Hannes Sthlin (Ergon Informatik AG, ETHZ)\\n\\n1645 - 1715: ST SESSION 4 \\nFrom research code to impact: 5 Years of RSE services for EPFL-ENAC\\nCharlie Weil (EPFL)\\n Or \\nSustaining Scientific Workflows: The Case for Stable RSE Roles in AiiDA Development\\nEdan Bainglass and Ali Khosravi (PSI)\\n\\n1715 - 1730 farewell\\n\\n1717 - 1909 commute Fribourg, Charmettes to Zurich hardbrucke\\n\\n[1557] [main 7bc62c3] log updates:1 file changed, 1 insertion(+), 1 deletion(-)\\n\\n[1647] I would like to create a piece of lightweight software that ingests a simple  self-documented CSV data file (called an iCSV or a NEAD) that contains both metadata (denoted by '[METADATA]') as well as dat (denoted by '[DATA]') with data fields noted in '[FIELDS]'\\n\\nThe NEAD format is a delimiter separated value (DSV) format similar to CSV, but with a header section, followed by the data section. in general it looks a bit like this:\\n\\n# NEAD 1.0 UTF-8\\n# [METADATA]\\n# station_id = 803027F4\\n# station_name = GC-NET GOES station Summit Station\\n# srid = EPSG:4326\\n# geometry = POINTZ (38.5053 72.5794 3199)\\n# nodata   = -999\\n# timezone = 0\\n# field_delimiter = ,\\n# [FIELDS]\\n# fields = timestamp,ISWR,OSWR,NSWR,TA1,TA2,RH1,RH2,VW1,VW2,DW1,DW2,P,HS1,HS2,V\\n# add_offset    = 0,0,0,0,273.15,273.15,0,0,0,0,0,0,0,0,0,0\\n# scale_factor  = 1,1,1,1,1,1,0.01,0.01,1,1,1,1,100,1,1,1\\n# units         = time,W/m2,W/m2,W/m2,C,C,%,%,m/s,m/s,,,mbar,m,m,V\\n# standard_name = timestamp_iso,short_wave_incoming_radiation,short_wave_outgoing_radiation,net_radiation,air_temperature_1,air_temperature_2,relative_humidity_1,relative_humidity_2,wind_speed_1,wind_speed_2,wind_direction_1,wind_direction_2,atmospheric_pressure,snow_height_1,snow_height_2,battery_voltage\\n# database_fields = timestamp_iso,swin,swout,netrad,airtemp1,airtemp2,rh1,rh2,windspeed1,windspeed2,winddir1,winddir2,pressure,sh1,sh2,battvolt\\n# database_fields_data_types = timestamp,real,real,real,real,real,real,real,real,real,real,real,real,real,real,real\\n# \\n# [DATA]\\n# \\n1996-05-12 11:00:00+00,356.6,288.29,-999,-999,-999,96.05,94.79,3.84,4.2,186.5,-999,691.7,-999,0.05,4.59\\n1996-05-12 12:00:00+00,489.3,453,-999,-999,-999,95.55,94.04,4.11,4.5,205.5,-999,691.8,-999,0.01,1.05\\n1996-05-12 13:00:00+00,622,506.87,-15.43,-999,-999,91.01,90.89,3.39,3.58,165.3,-999,692,-999,0,0\\n1996-05-12 14:00:00+00,684.2,569.11,15.51,-999,-999,87.46,88.67,5.36,5.61,217.6,-999,692.2,-0.01,-0.01,12.69\\n1996-05-12 15:00:00+00,680.6,572.57,-90.87,-999,-999,86.25,87.5,6.82,7.13,222.3,-999,692.6,-0.01,-0.01,12.73\\n1996-05-12 16:00:00+00,674.6,569.3,-137.32,-999,-999,87.05,87.38,5.51,5.77,219.6,-999,692.6,0,0,12.78\\n1996-05-12 17:00:00+00,620.2,528.53,-157.64,-999,-999,88.73,89.67,5.78,6.06,220.9,-999,692.8,0,0.01,12.69\\n1996-05-12 18:00:00+00,507.6,435.89,-117.4,-999,-999,90.03,91.31,5.84,6.12,221.3,-999,693.1,0,0.01,12.64\\n1996-05-12 19:00:00+00,406.8,350.35,-61.34,-999,-999,91.13,92.28,5.83,6.1,229.2,-999,692.9,0,0.01,12.61\\n1996-05-12 20:00:00+00,366.8,319.41,-77.03,-999,-999,91.24,92.4,7.06,7.37,240.2,-999,693,0,0,12.54\\n1996-05-12 21:00:00+00,275.8,241.88,-92.72,-999,-999,92.66,93.76,4.87,5.16,237.9,-999,693,0,0,12.44\\n\\n\\nan iCSV looks very similar, something like the following:\\n\\n# iCSV 1.0 UTF-8\\n# [METADATA]\\n# field_delimiter = |\\n# geometry = POINTZ(781957.771521 185571.662370 1556.700000)\\n# nodata = -999.000000\\n# srid = EPSG:21781\\n# station_id = ANU564\\n# timezone = 1.000000\\n# [FIELDS]\\n# fields = timestamp|DW|ISWR|P|PSUM|RH|TA|VW_U|VW_V|VW_MAX\\n# add_offset    = 0|0|0|0|0|0|0|0|0|0\\n# standard_name = timestamp_iso|wind_direction|short_wave_incoming_radiation|atmospheric_pressure|total_precipitation|relative_humidity|air_temperature|wind_speed_u_component|wind_speed_v_component|wind_speed_maximum\\n# [DATA]\\n2005-08-23T15:30:00|-999|50|83700|0|0.889|281.94|1.5|-999|2.8\\n2005-08-23T16:30:00|-999|47|83700|0|0.913|282.05|1.21|-999|3.2\\n2005-08-23T17:30:00|-999|42|83700|0|0.897|281.76|1.39|-999|3.5\\n2005-08-23T18:30:00|-999|21|83800|0|0.913|281.7|0.628|-999|1.9\\n2005-08-23T19:30:00|-999|3|83800|0|0.931|281.52|0.628|-999|1.5\\n2005-08-23T20:30:00|-999|3|83800|0|0.942|281.31|0.553|-999|1.4\\n2005-08-23T21:30:00|-999|3|83900|0|0.941|281.12|0.47|-999|1.1\\n2005-08-23T22:30:00|-999|3|83900|0|0.945|280.99|0.565|-999|1.3\\n2005-08-23T23:30:00|-999|3|83900|0|0.947|280.85|0.464|-999|1.3\\n2005-08-24T00:30:00|-999|3|83900|0|0.945|279.81|0.435|-999|1.1\\n2005-08-24T01:30:00|-999|3|83900|0|0.981|279.3|0.576|-999|1.4\\n2005-08-24T02:30:00|-999|3|83800|0|0.992|278.73|0.674|-999|1.7\\n2005-08-24T03:30:00|-999|3|83800|0|1|278.51|0.472|-999|1.3\\n2005-08-24T04:30:00|-999|3|83800|0|1|277.88|0.629|-999|1.5\\n2005-08-24T05:30:00|-999|3|83800|0|1|277.58|0.436|-999|1\\n2005-08-24T06:30:00|-999|32|83800|0|1|277.1|0.517|-999|1.1\\n2005-08-24T07:30:00|-999|227|83800|0|0.965|278.34|0.8|-999|1.7\\n2005-08-24T08:30:00|-999|461|83800|0|0.853|281.08|0.929|-999|2.2\\n2005-08-24T09:30:00|-999|650|83800|0|0.736333|283.63|0.883|-999|1.5\\n2005-08-24T10:30:00|-999|783|83800|0|0.629|286.25|2.08|-999|4.3\\n\\naccording to their notation, the createors of the self documented CSV's have this to say about formatting their metadata\\n\\nNEAD                  ::= firstline header_section data_section\\nfirstline             ::= '#' ' ' 'NEAD' ' ' version_number ' ' file_format newline\\nheader_section        ::= metadata_header metadata_section fields_header fields_section\\n\\n/* metadata */\\nmetadata_header       ::= '#' ' ' '[METADATA]' newline\\nmetadata_section      ::= (('#' ((whitespace (required_metadata |\\n                                              recommended_metadata |\\n                                              other_metadata))? )? lineend ) | newline)+\\nrequired_metadata     ::= ('field_delimiter' assignment field_delimiter) |\\n                          ('geometry' assignment geometry) |\\n                          ('srid' assignment EPSG_code)\\nrecommended_metadata  ::= ('station_id' assignment alphanumeric)\\nrecommended_metadata  ::= ('timestamp_meaning' assignment timestamp_meanings)\\nrecommended_metadata  ::= 'nodata' assignment (integer | float)\\nrecommended_metadata  ::= 'timezone' assignment (integer | float | tz_string)\\nrecommended_metadata  ::= ('doi' | 'reference') assignment value\\nother_metadata        ::= key assignment value\\n\\n/* fields */\\nfields_header         ::= '#' ' ' '[FIELDS]' newline\\nfields_section        ::= (('#' ((whitespace ( required_fields |\\n                                               recommended_fields |\\n                                               other_fields ))? )? lineend ) | newline)+\\nrequired_fields       ::= ('fields' assignment values)\\nrecommended_fields    ::= ('units_multiplier' |\\n                           'units_offset' |\\n                           'units' |\\n                           'long_name' |\\n                           'standard_name') assignment values\\nrecommended_fields    ::= 'timestamp_meaning' assignment timestamp_meanings\\nother_fields          ::= key assignment values\\n\\n/* data */\\ndata_section          ::= '#' ' ' '[DATA]' newline dataline+\\ndataline              ::= (value ( field_delimiter value )* newline)\\nvalues                ::= value ( field_delimiter whitespace value )*\\n\\n/*\\n  NOTE:\\n  All values must be the same length.\\n  This means everything in the \\\"[FIELDS]\\\" section maps 1:1 to the columns in the \\\"[DATA]\\\" section.\\n*/\\n\\n\\n/* other */\\nkey             ::= char (alphanumeric*)?\\nvalue           ::= unicode-char*\\nassignment      ::= whitespace? '=' whitespace?\\nfield_delimiter ::= [,|\\\\/:;]\\nversion_number  ::= digit* ('.' (alphanumeric)+)? ('.' (alphanumeric)+)?\\nfile_format     ::= 'UTF-8' | 'ASCII'\\nEPSG_code       ::= 'EPSG' ':' digit digit digit digit\\ngeometry        ::= 'POINT(' float float ')' |\\n                    'POINTZ(' float float float ')' |\\n                    WKT_string |\\n                    column_name\\ntimestamp_meanings ::= 'beginning' | 'end' | 'middle' | 'instantaneous' | 'other' | 'undefined'\\n\\n/* generic */\\nwhitespace      ::= (tab | space)+\\ncomment         ::= '#' whitespace? (unicode-char)?\\nlineend         ::= (whitespace | comment)? newline\\nnewline         ::= #x0A\\ntab             ::= #x9\\nspace           ::= #x20\\nchar            ::= [a-zA-Z]\\ndigit           ::= [0-9]\\ninteger         ::= [+-]? digit+\\nalphanumeric    ::= (digit | char)+\\nhex             ::= (digit | [a-fA-F])+\\nfloat           ::= integer '.' ((digit)+)?\\n\\n/* any Unicode character, excluding the surrogate blocks, FFFE, and FFFF. */\\nunicode-char    ::= #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]\\n\\n\\nas you can see, there is a bit of a challenge in making sure that the metadata are complete and match the data in a useful way. However,the metadata also provide us a really neat way of checking the data for completeness. \\n\\ntherefore, i would like to use the metadata in the selfdocumented csv to quality check the data using the frictionless framework (https://framework.frictionlessdata.io/) to create a schema (https://framework.frictionlessdata.io/docs/framework/schema.html) and then use that schema to check the [DATA] section. \\ni will research the frictionless framework, as well as best practices for lightweight, simple software design, and then write python code that \\n1. ingests an iCSV or NEAT file\\n2. checks the metadata against the standards laid out above\\n3. uses the metadata section to create a schema (and save the schema in the same file as the data)\\n4. uses the schema (in addition to the baseline check: https://framework.frictionlessdata.io/docs/checks/baseline.html) to check the data. \\n\\nideally these are all separate scripts that can be run in sucession. a sort of modular design. each module will produce an output file with human-readable error messages showing what aspects need to change and how to change them (can rely on the programming of frictionless). \\n\\nto be clear, I would like a working python script that can be run by placing a datafile in the same file structure (called data.icsv), and when i run the script, txt files will be created with either an indication that everything looks good, or clear descriptions of what needs to change.\\n\\n[1652] next to write a script that:\\n1. Parses any NEAD or iCSV file provided via the command line.\\n2. Validates required metadata and fields according to the documented specification.\\n3. Uses the metadata to generate a Frictionless Data schema.\\n4. Performs baseline and schema-based validation using the Frictionless framework.\\n5. Outputs human-readable `.txt` reports for each stage of validation.\\n\\n[1701] uploading to git\",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":6,\"date\":\"2025-09-02T00:00:00Z\",\"title\":\"2025_09_02_Tuesday\",\"content\":\"[0755] answering and archiving messages\\n[0802] starting work on the NEAD validator\\n[0814] The program is functioning - but perhaps not optimally. it is currently split into three steps:\\nStep 1: Check metadata\\n```bash\\npython3 check_metadata.py data.icsv\\n```\\n Creates `metadata_report.txt`.\\nStep 2: Generate schema\\n```bash\\npython3 create_schema.py data.icsv\\n```\\n Creates `data_schema.json` and `schema_report.txt`.\\nStep 3: Validate data\\n```bash\\npython3 validate_data.py data.icsv\\n```\\n Creates `data_report.txt`.\\nAfter each step, you can open the reports:\\n```bash\\ncat metadata_report.txt\\ncat schema_report.txt\\ncat data_report.txt\\n```\\nThey will either say **OK** or list specific errors/warnings.\\n\\nThe challenge right now is that the the schmea is simplistic, and yet is causing errors in the data checks. \\nfor example:\\n\\nfor the data:\\n# iCSV 1.0 UTF-8\\n# [METADATA]\\n# field_delimiter = |\\n# geometry = POINTZ(781957.771521 185571.662370 1556.700000)\\n# nodata = -999.000000\\n# srid = EPSG:21781\\n# station_id = ANU564\\n# timezone = 1.000000\\n# [FIELDS]\\n# fields = timestamp|DW|ISWR|P|PSUM|RH|TA|VW_U|VW_V|VW_M\\n# add_offset    = 0|0|0|0|0|0|0|0|0|0\\n# standard_name = timestamp_iso|wind_direction|short_wave_incoming_radiation|atmospheric_pressure|total_precipitation|relative_humidity|air_temperature|wind_speed_u_component|wind_speed_v_component|wind_speed_maximum\\n# [DATA]\\n2005-08-23T15:30:00|-999|50|83700|0|0.889|281.94|1.5|-999|2.8\\n2005-08-23T16:30:00|-999|47|83700|0|0.913|282.05|1.21|-999|3.2\\n2005-08-23T17:30:00|-999|42|83700|0|0.897|281.76|1.39|-999|3.5\\n2005-08-23T18:30:00|-999|21|83800|0|0.913|281.7|0.628|-999|1.9\\n2005-08-23T19:30:00|-999|3|83800|0|0.931|281.52|0.628|-999|1.5\\n2005-08-23T20:30:00|-999|3|83800|0|0.942|281.31|0.553|-999|1.4\\n2005-08-23T21:30:00|-999|3|83900|0|0.941|281.12|0.47|-999|1.1\\n2005-08-23T22:30:00|-999|3|83900|0|0.945|280.99|0.565|-999|1.3\\n2005-08-23T23:30:00|-999|3|83900|0|0.947|280.85|0.464|-999|1.3\\n2005-08-24T00:30:00|-999|3|83900|0|0.945|279.81|0.435|-999|1.1\\n2005-08-24T01:30:00|-999|3|83900|0|0.981|279.3|0.576|-999|1.4\\n2005-08-24T02:30:00|-999|3|83800|0|0.992|278.73|0.674|-999|1.7\\n2005-08-24T03:30:00|-999|3|83800|0|1|278.51|0.472|-999|1.3\\n2005-08-24T04:30:00|-999|3|83800|0|1|277.88|0.629|-999|1.5\\n2005-08-24T05:30:00|-999|3|83800|0|1|277.58|0.436|-999|1\\n2005-08-24T06:30:00|-999|32|83800|0|1|277.1|0.517|-999|1.1\\n2005-08-24T07:30:00|-999|227|83800|0|0.965|278.34|0.8|-999|1.7\\n2005-08-24T08:30:00|-999|461|83800|0|0.853|281.08|0.929|-999|2.2\\n2005-08-24T09:30:00|-999|650|83800|0|0.736333|283.63|0.883|-999|1.5\\n2005-08-24T10:30:00|-999|783|83800|0|0.629|286.25|2.08|-999|4.3\\n\\nand the schema:\\n{\\n  \\\"fields\\\": [\\n    {\\n      \\\"name\\\": \\\"timestamp\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"DW\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"ISWR\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"P\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"PSUM\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"RH\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"TA\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"VW_U\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"VW_V\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"VW_M\\\",\\n      \\\"type\\\": \\\"string\\\"\\n    }\\n  ],\\n  \\\"missingValues\\\": [\\n    \\\"-999.000000\\\"\\n  ]\\n}\\n\\nwe get the error report:\\nData validation errors:\\n  Row ? Field 1: Label \\\"2005-08-23T15:30:00\\\" in field timestamp at position \\\"1\\\" does not match the field name in the schema\\n  Row ? Field 2: Label \\\"-999\\\" in field DW at position \\\"2\\\" does not match the field name in the schema\\n  Row ? Field 3: Label \\\"50\\\" in field ISWR at position \\\"3\\\" does not match the field name in the schema\\n  Row ? Field 4: Label \\\"83700\\\" in field P at position \\\"4\\\" does not match the field name in the schema\\n  Row ? Field 5: Label \\\"0\\\" in field PSUM at position \\\"5\\\" does not match the field name in the schema\\n  Row ? Field 6: Label \\\"0.889\\\" in field RH at position \\\"6\\\" does not match the field name in the schema\\n  Row ? Field 7: Label \\\"281.94\\\" in field TA at position \\\"7\\\" does not match the field name in the schema\\n  Row ? Field 8: Label \\\"1.5\\\" in field VW_U at position \\\"8\\\" does not match the field name in the schema\\n  Row ? Field 9: Label \\\"-999\\\" in the header at position \\\"9\\\" is duplicated to a label: at position \\\"2\\\"\\n  Row ? Field 10: Label \\\"2.8\\\" in field VW_M at position \\\"10\\\" does not match the field name in the schema.\\n\\nI think is is beacause the schema is not being created correctly. so I will start there. Perhaps it will be the case that I need to actually go back farther in the code in order to see if the \\\" no errors\\\" code is actually error free. But that can come later. \\n[0826] let's dive into the code for 'create_schema.py':\\n#!/usr/bin/env python\\n# create_schema.py: Use metadata to generate a Frictionless schema.\\n\\nimport sys\\nimport json\\nfrom frictionless import Schema, fields\\n# Import the same parse function as above\\nfrom check_metadata import parse_metadata, check_metadata\\n\\ndef build_schema(metadata, fields_meta):\\n    names = fields_meta[\\\"fields\\\"]\\n    types = fields_meta.get(\\\"database_fields_data_types\\\", [])\\n    schema_fields = []\\n    for i, name in enumerate(names):\\n        dtype = types[i] if i < len(types) else \\\"\\\"\\n        dtype = dtype.lower()\\n        if \\\"timestamp\\\" in dtype or \\\"date\\\" in dtype:\\n            field = fields.DatetimeField(name=name)\\n        elif dtype in (\\\"real\\\", \\\"float\\\", \\\"double\\\"):\\n            field = fields.NumberField(name=name)\\n        elif dtype in (\\\"integer\\\", \\\"int\\\"):\\n            field = fields.IntegerField(name=name)\\n        else:\\n            field = fields.StringField(name=name)\\n        schema_fields.append(field)\\n    schema = Schema(fields=schema_fields)\\n    # Add missing (nodata) value if provided\\n    nodata = metadata.get(\\\"nodata\\\")\\n    if nodata:\\n        schema.missing_values = [str(nodata)]\\n    return schema\\n\\ndef main():\\n    infile = sys.argv[1] if len(sys.argv) > 1 else \\\"data.icsv\\\"\\n    metadata, fields_meta = parse_metadata(infile)\\n    # Reuse metadata checks to ensure fields list exists\\n    errors = check_metadata(metadata, fields_meta)\\n    out_report = \\\"schema_report.txt\\\"\\n    if errors:\\n        with open(out_report, \\\"w\\\") as report:\\n            for err in errors:\\n                report.write(f\\\"ERROR: {err}\\\\n\\\")\\n            report.write(\\\"\\\\nCannot build schema until metadata issues are resolved.\\\\n\\\")\\n        print(f\\\"Schema generation aborted. See {out_report} for errors.\\\")\\n        return\\n    schema = build_schema(metadata, fields_meta)\\n    # Save schema JSON\\n    with open(\\\"data_schema.json\\\", \\\"w\\\") as f:\\n        json.dump(schema.to_descriptor(), f, indent=2)\\n    with open(out_report, \\\"w\\\") as report:\\n        report.write(\\\"OK: Schema created successfully.\\\\n\\\")\\n    print(f\\\"Schema written to data_schema.json. Report: {out_report}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\\n[0827] In here we are using the frictionless framework to create a schema, but we are mostly just looking at the data types in the first block (i.e. is it a date, is it a number, etc.)\\n[0847] I think instead, we need to reconfigure this code in order to make a schema that is more in line with what frictionless usually sees. For example, the example schemas that I have access to look something like this:\\n[0909] figured out the PSI email puzzle: the password given to us must be changed, but they never specified where or how. So I found pw.psi.ch, which permits you to enter the original password information to register and unlock the account. it also allows you to reset your password to grant access to the PSI services. \\n[0912] now working on the PSI VPN puzzle\\n[0950] now have a working email (chase.nunez@psi.ch) as well as login (nunez_c) for PSI services. \\n[0953] sending confirmation emails to Bobby N and Claudia N re: two factor authentication \\n[1010] figuring out about whether we will go to PSI tomorrow, or not. \\n[1033] [master 18f8edb] updating the schema creation to be less flat, and more accommodating of data complexity. Also updating the validate script to accommodate the changes in the shema script. 2 files changed, 126 insertions(+), 89 deletions(-)\\n[1035] [main 9604e0e] log updates 1 file changed, 1 insertion(+), 1 deletion(-)\\n[1049] creating a wrappter to run all the scripts at once: \\nRun validation on one file: python3 nead_validate.py data.icsv\\nRun validation on multiple files:python3 nead_validate.py file1.icsv file2.icsv file3.icsv\\n[1056] [master 5da0eec] fixing validation script, as well as including the output files for example 6 files changed, 221 insertions(+), 19 deletions(-)\\n[1116] tried to create a file structure, but messed it up, so reverted back to the commit above, and then deleted the output files. \\n[1122] [master 83b90eb] chnaging error reporting 2 files changed, 3 insertions(+), 3 deletions(-)\\n[1124] [main c8a7081] log updates 1 file changed, 1 insertion(+), 1 deletion(-)\\n[1138] leaving for lunch\\n[1219] launching a new roku session to check through the metadata generator that EK has been working on.\\n[1228] the metadata generator is great: some features that would be time-intensive to implement, but potentially useful would be the ability to upload an existing json ad then edit the details using the gui, or perhaps being able to fill out multple separate metadata files sequentially with some shared data. \\n[1308] updating the metadata and ReadMe info for the rdm_nav project\\n[1354] updating the documentation and file structures on gitlab, preparing for jour fixe this afternoon.\\n[1405] now adding citation information to the read me and other files\\n[1416] working on the Zenodo guide for Michel and the DORA team. Once I am done, I will hand it over to FM and have him look over it with fresh eyes.\\n[1447] updating gitlab projects, project information, and merge requests. \\n[1520] jour fixe: FM will reach out to Florain Altermann's AI grant pitch\\n[1547] updating FF on on envidat_frictionless and RDM_nav\\n[1548] FM updates FM on the DMP program updates: a list of questions and stored answers. A list of elements that are knot intp a markdown document. can be exported as LaTeX, markdown, or txt\\n[1553] need to look for existing tools that Felix could either improve on Data Stewardship Wizard (on one of Stuart's servers\\n[1556] ask anush for guide on Zenodo for DOI's \\n[1634] looking to move my personal file to the server for data backup purposes.\\n[1658] migrated all filed from desktop to g_EResources > 06_Projects > ORD_RDM > RDM_Personnel > C_NUNEZ\\n\\ntomorrow need to email Ionut\\nneed to fix RDM_Nav \\n\",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":7,\"date\":\"2025-09-03T00:00:00Z\",\"title\":\"2025_09_03_Wednesday\",\"content\":\"[0755] email responses to Lothar, as well as EK\\n[0807] need to work on the navigation graphic, and to send a message to Ionut regarding a meeting for the EnviDat frictionless program. \\n[0823] drafted an email to Ionut, but will wait to hear from El and Felix before sending.\\n[0833] getting a message out of quarantine from EK, about drupal and the website hosting the interactive graphics through an iframe, hosted on gitlab. \\n[0841] working on getting my git branches displayed, since I think now that I have working versions of both the RDMnav as well as the Frictionless app, I can at least branch and come back to them.\\n[0900] email sent to Ionut with input fro FM and EK\\n[0922] tagging my most recent commits as \\\"v1.0\\\" for both the rdm_nav project as well as the envidat_qc project and then branching in to a developer branch for both. In the case of the rdm_nav project, i have called it FF_sugg becuase it will be incorperating many of FF's suggestions about the navigation. in the case of the envidata_qc project, i have called it fix_data_qc because it will be primairly a way of fixing the error reporting over sensitivity of the the final module of that program. \\n[1026] organizing the logistics for this week and next in regards to where we are working.Tomorrow we re going to Fribourg for a conference on research software engineering, then next wednesday september 10th we are headed to wsl, then on thursday september 11th we are headed to PSI to pick up our badges. emails have been sent to WSL and PSI, as well as internally to felix and to el\\n[1029] tagging the tjournal for commits at the beginning of the month as well\\n[1036] I think I am going to create a new level two section in the \\\"share\\\" branch to group some of the library services. i.e. scientific writing, literature management, plagerisms check, and llm's and ai  all fall under writing. scientific publishing, open access, copyrite, get a doi all fall under scientific publishing, and then increasing impact and bibliometrics both fall under impact\\n[1112] i have added a level 2/3 to the sharing section (writing, publishing, impact). but the discover section is a little bare.\\n[1126] [FF_sugg 2e1a259] adding a layer to the share section so that its terminal branches are now housed in piblishing, writing, and impact. 1 file changed, 275 insertions(+), 261 deletions(-)\\n[1144] leaving for lunch\\n[1210] back from lunch, meeting with FM and JR\\n[1233] emails and messages\\n[1242] looking up how to change the colors on the graphic, but actually thinking now that it would answer FF's request, and also be better design, to get rid of the third layer of the sunburst, and instead to create 13 new pages that synthesize the 4RI advice on different topics instead of linking out. \\n[1305] for some reason or another the code has been updated and pushed to github, but when it comes to visualize the chnages in a pivate window (to prevent cookies from ignoring chnages) there graph is mysteriiouysly unmoved by the coding changes. \\n[1306] I am trying now to make sure that the git version is telling me the truth.\\n[1313] ok i have solved that problem, with both literature and data search being represented (and both need to have a size specified). I have also reconfigured the colors a little bit. \\n[1331] the graph now only goes to two layers deep via flters on the depth part of the code. My pan now is to 1) get the diagram working on the website, and 2) create a page/subset of pages that can illustrate what each of these areas would link out to. i.e. there would be a page for DMP templates that would both synthesize what the gerenal topic was, lib4RI's services synthesizing those resources, as well as specific links out to institute specific services. \\n[1341] [FF_sugg c1e7f85] adding chnages for the level filtering, allowing a more compact sunburst, but also requiring synthesis pages to be made and linked to 2 files changed, 8 insertions(+), 7 deletions(-)\\n[1423] helping el with some data sharing ideas in switch drive and slaying a host of merge requests that were autoinstantiated when i created a project from a template \\n[1443] learning now about git, it's development, and source code\\n[1512] reading about the source code of linux and git and linus torvald, who is a diver, among other things.\\n[1530] now reading about magic numbers (computer programing) as well as unix \\n[1553] i think my github account has not been transformed at the tinstance level to be a developer level account. perhaps this is a response to my inquiry to stuart in IT, but perhaps it is unrealted and just happens to every account when certain benchmarks are achieved?\\n[1603] sending an email to license.management@eawag.ch to get a zoom license for our meeting with Ionut next week\\n[1615] sending ionut a email, with a zoom link, and a link to the gitlab, but keeping it in drafts per EK's suggestion\\n[1659] planning trip to Fribourg tomorrow with EK. \",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":8,\"date\":\"2025-09-04T00:00:00Z\",\"title\":\"2025_09_04_Thursday\",\"content\":\"[0801] answering emails from PSI and EAWAG re PSI next week\\n[0819] train to HB > Fribourg > enhanceR conference. on the train working on the updates to the RDM nav code offline to save data usage\\n[0903] coding is going fine, but i fear that checking the html via the python3 -m http.server 8000 / web browser option does not work with no internet connection sadly\\n[0907] working now on the envidat readme file. \\n[1012] introduction to keynote: Imanol Schlag, ETH AI center: Apertus: Decmocrativzing open and complianrt LLMS for global Language environments\\n[1013] ?Transformer model for introduction of novel infomrmation\\n[1019] performance scales with parameter count and training dataset size, because parameters are unifgofmred\\n[1022] preference training for the provisioning of human values on answers\\n[1023] reinforncement learning from verifiable rewards: a task wherer you can easily judge if the response of the model has been correct, then you let the model reason it out, and you reinfoce all the chouces that led to the correct model\\n[1024] supercomputers have a lifespan of about 6years, becuase of the costs of energy and the increase in effeceiency\\n[1025] ALPS is a hopper generation that has actually increased in value since it was inogurated becuase of the rise fo AI\\n[1026] ALPS is 7th on the top 500 list, majority of the top 10 are using AMD (doesnt have software stack), some have intel, EAGLE and ALP use NVIDIA\\n[1027] Jlich is coming up and will be doulne the size of ALPS, but having toruble getting up adnd runing\\n[1030] 15M compute hours beingn distributed by research calls\\n[1032] LLMS increasing in quality and ubiquity, but is i for the better: untransparent\\n[1034] APERTUS: train on data that is copyright protected, but honor opt outs retroactively, trained on over 1000 languages, \\n[1027] 2 models, 1 8B parameters, 1 at 70 B parameters, 15 trillion tokens of text, 4096 GPU's on alps, but stil  10x smaller than the largest models (700B) not a replace ment for deepskek or chatgpt, but can speak romansch and swissgerman (reasonable well)\\n[1040] Deployment: AWS and Azure make available on platforms.  a free chatbased interface provided by swisscom\\n[1044] questions of safety, best possible outcome for Apertus, preference training /political and moral training, where can APERTUS be used in the ETH domain?, will intermediate steps be released? yes, what is thre make-up of the group? mostly swiss reseachers, quantized models released? maybe was the domain of someone who left too early;\\n[1115] How to use rust on order to create a domain specific language: juson yu (PSI)  ,usually nota good idea to reate a new language, so why create a new language: RUST: a effecient declarative workflow language\\n[1135] it has become clear that this is not an instruction for how to build your own language, but rtaher about the language that he has created in rust. \\n[1136] has me thinking about the simple program idea I had earlier that I could work on:\\nhow to create a program that is 1) based in the terminal, i.e a terminal is started, a program is started by typing 'captainslog'. A simple grpahical interface is creaated in a new terminal window. it is simple, blank, with a small outline box with the current time (YYYY_MM_DD_HH_MM) in square brackets followed by a colon (:). The user makes a note (e.g: 'hello world, i am here') and hits enter.\\nonce enter has been pressed, the text (including the exact date and time of pressing enter is saved in to one .json file that collects all log entries. \\nThen, once the new entry has been added to the .json file, the .json file is saved, and the containing folder (a git repository) has the changes automaticalled added (git add -A) and committed (git commit -m\\\"log updates\\\") and pushed (git push) and automatically log in via a token.\\nthen, a new blank box is ready for a new entry. \\nideally the program will also have a search function for dates where the user can submit a YYYY_MM_DD and pull up all the entries for that day.\\nideally there would also be a simple tag function where the user can use a #KEYWORD in the text that can also be searched by the user later. \\n[1142] https://rs4rse.github.io/\\n[1145] questions about the purpose and how of the language: a way of managing \\n[1146] handover to the next talk: The AFFORD workflow to create your own data index with Git, R and Quarto\\n[1147] Gorka Fraga Gonzalez UZH, Center for Reproducable Science and Research Synthesis\\n[1149] avoiding the open research data dump for biomedical research, inderciplinary and multicenter, focus on synchotron, need to minimize dependencies in the medium long term. \\n[1153] three compoents: 1.dynamic doc generation, 2. git, software containerization; uses quarto integrated into R studio\\n[1157] so as far as i understand it, he is just talking about using quarto for writing a markdown document that is published on a gitlab page that is visible to all collaborators\\n[1159] ?docker deep dive on the computational infrastructure that facilitate the container, what are the possibilites what are the alternatives\\n[1202] a simple way for researhers who are not as tech savvy have better FAIR practices with their data\\n[1205] a bottleneck getting different groups to use the same labguage in metadata, and then also trying to get the PI's to use the services. the most important part is getting the metadata published in zenodo\\n[1208] https://osf.io/preprints/metaarchiv/64fch_v1\\n[1209] questions regarding if ew can take away the task of ontologies from the groups, and perhaps have them taken by AI? this project could be called SYNONYMS where the ontologies are determined by the similar usage of different groups\\n[1326] Renku Workshop: mostly just an general introduction with people circlig around to help people set up an account, The yhave mostly issued a link to do most of the training. \\n[1345] pulling data from gitlab via the terminal in Renku:\\n61f377d..2d19885  master      -> origin/master\\n * [new branch]      fix_data_QC -> origin/fix_data_QC\\nUpdating 61f377d..2d19885\\nFast-forward\\n NEAD_Validator/check_metadata.py                              |  70 ++++++++\\n NEAD_Validator/create_schema.py                               |  96 +++++++++++\\n NEAD_Validator/data.icsv                                      |  33 ++++\\n NEAD_Validator/data_data_report.txt                           |  30 ++++\\n NEAD_Validator/data_schema.json                               |  70 ++++++++\\n NEAD_Validator/metadata_report.txt                            |   1 +\\n NEAD_Validator/nead_validate.py                               |  63 +++++++\\n NEAD_Validator/validate_data.py                               |  58 +++++++\\n README.md                                                     | 188 ++++----------------\\n data/input/README.txt                                         |   2 +-\\n data/input/{biomass_summary_errors.csv => biomass_sample.csv} |   0\\n envidat_icsv_nead_metadata.ipynb                              | 762 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++--------\\n example.ipynb                                                 | 196 ---------------------\\n requirements.txt                                              |   0\\n trial/data/output.icsv                                        |  11 ++\\n trial/output.icsv                                             |  25 +++\\n trial/validate_icsv.py                                        | 405 +++++++++++++++++++++++++++++++++++++++++++\\n workflows/my-workflow.yaml                                    |  37 ----\\n 18 files changed, 1583 insertions(+), 464 deletions(-)\\n[1419] using gradio to make open applications, example using protein structure\\n[1420] Gradio: what is is, hugging face spaces, API to make it FAIR\\n[1422] gradio mainly developed for machine learning image analysis, becuase protein analysis is basically machine learning now\\n[1423] streamlit, colab,  as an alternative to gradio, but gradio is more interoperable (no api in streamlit) \\n[1427] where to host: huggingface.com (also hosting apertus)\\n[1428] huggingface is also a repository for models and data sets that can be browsed? sponsors some free GPU hours for compute resources\\n[1430] gradio also permits custom components, you can buld it using Svelte, and then you can publish to the package index. You can visualize the data once, and then the researchsers can continue to use to tool. neat!\\n[1435] he has used it to visualize protein structures in different ways. \\n[1437] Rest API: interoperable and reusable. automatically usable using huggingface\\n[1439] can add arbitrary html for interfaces, and also has the option to add a generator to save on compute time. \\n[1440] 2 cpu cores, and 16 GB ram for free. also available: zero GPU only for graio spaces. \\n[1443] create web apps for almost any purpose, opensource, run locally, server, or cloud, huggingface makes it easy to share the app, REST spi makes it open\\n[1444] questions: what about scalability? - a queue/ scheduling system \\n[1532] ending keynote (scaling jupyter to millions of users (sylvan corlay) scientists need tools they can comprehend, dissect, and trust. \\n[1538] 15 people responsible for jupyter notebook (15 million notebooks on github)\\n[1540] ?binder deployment, a collection of jupyter notebooks (used by LIGO)\\n[1541] the scientific paper is obsolete - but computational support is har to provide at scale\\n[1542] see atlantic article by James Somers, and Paul Romer\\n[1544] using jupyter lite, see numpy.org, or capytale: a french high school deployment of jupyter\\n[1554] the challenege with packagin for webassembly is Fortran - \\n[1600] collaborative editing in the browser has transformed how we work togeather, and this should be available to all data and editing tyes (video, audio, CAD)\\n[1602] this turns out to be really difficult, and jupytercad (2022) turned out to be the template for a lot of different editing\\n[1603] JupyterGIS was able to be built in less than a quarter of the time. both work in jupyter lite\\n[1606] what about the future? jupyter lyte is still missing colaboartive editing. story maps coming to jupyterGIS. also grading and comments for student projects. \\n[1607] pyData paris 2025 September 30 - october 1, Apache arrow summit Otober \\n[1610] is there access to client GPU? no. \\n[1611] web assembly is the future.\\n[1617] PoC or Prod: case studies about how ai can be used and misused - emphasizes the importance of having a good idea of the problem and of the implications of getting it wrong (validation)\\n[1647] Lessons learned from a team of RSE's: Charlie Weil:EPFL\\n[1710] lvs for train\\n[1735] drafting email to Ingo, Ionut, etc. to be sent tomorrow\\n[1806] reading the arxiv preprint of https://osf.io/preprints/metaarchiv/64fch_v1\\n[1902] trying to get access to the stage to set up iframe\",\"tags\":[\"enhanceR\",\"Fribourg\"],\"priority\":0},{\"id\":9,\"date\":\"2025-09-05T00:00:00Z\",\"title\":\"2025_09_05_Friday\",\"content\":\"need to put the web app RDM_nav on the website which will invbolve asking janina to give you a new credentaisl that you can actually remember\\nsend an email to stuart\\npick health insurer\\ncall wise and transfer money out\\nemail and switch wed at PSI and WSL Thursday?\\n\\na note for the future: this was, in fact, a full day of work at EAWAG, but i was working on a new notetaking program that cant run at the same time, and so all of kmy updates are in stardate, and not here. \\nother information about what i was up to and when can be found as commits on my eawag gitlab. \\n\",\"tags\":[\"EAWAG\"],\"priority\":0},{\"id\":10,\"date\":\"2025-09-08T00:00:00Z\",\"title\":\"2025_09_08_Monday\",\"content\":\"[0854] back i the office and excited to keep working on stardate after I answer all these emails and send off a handful of change of date things\\n[0910] answered all emails now on to chnaging our schedule so that we are in PSI on wednesday the 10th and here for a meeting with Ionut on Thuesday\\n[0920] sent emails updating Ionut, Claudia N, Veronica V, Susanne G, Elle and Felix\\n[0921] thinking now about updating the stardate program, need to add a few features\\n[0923] FF is in the office this morning. \\n[0925] I need to fix the access token for github so that stardate can update automatically. \\n[0955] ok so I have now got stardate up and running and will be switchig to that to see if it can function as a daily driver. \\n[0956] i will be pasting all the entries from this log into that one\\n\",\"tags\":[\"EAWAG\"],\"priority\":0}]",
      "tags": [
        "table",
        "row",
        "cell",
        "table",
        "row",
        "cell",
        "table",
        "row",
        "table",
        "row",
        "cell",
        "table",
        "row",
        "cell",
        "table",
        "row",
        "cell",
        "table",
        "row",
        "cell",
        "table",
        "row",
        "cell",
        "table",
        "row",
        "cell",
        "table",
        "row",
        "cell",
        "scrollto",
        "schema",
        "schema",
        "writing",
        "set",
        "set",
        "x0a",
        "x9",
        "x20",
        "x9",
        "xa",
        "xd",
        "x20",
        "xd7ff",
        "xe000",
        "xfffd",
        "x10000",
        "x10ffff",
        "keyword"
      ]
    },
    {
      "timestamp": "2025_09_08_10_05_07",
      "text": "so now that stardate is in a stable format. I think i can focus on other aspects of this week. Namely preparing for the meeting with ionu on thursday, my trip to PSI on wednesday, my jour fixe on tuesday, and the DCC training today at 1400.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_10_07_23",
      "text": "i think that means trying to get the updated RDM_nav as well as the supporting pages up and running on the stage",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_10_12_33",
      "text": "adding some development ideas to the read me. might make a new branch that develops some new features in stardate.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_10_24_48",
      "text": "attempting to make edits to the drupal website stage site, but i am receiving the error: \"The website encountered an unexpected error. Try again later.\"",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_10_36_03",
      "text": "I have messaged Federico about this error, but effectively cant do any tasks related to the RDM nav until he gets back to me.",
      "tags": []
    },
    {
      "timestamp": "2025_09_08_11_00_08",
      "text": "here is my current entry after the update with the location entry (seems to work eithout fault, as well as the box around the entry window - hwever there is a gap in the box at the right for some reason. perhaps it is intentional. I will have to see if this location data has also messed up the json file.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_11_03_48",
      "text": "fixed the upstream branch problem for the v2 features and wondering if that will fix the aut commits",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_11_04_54",
      "text": "ah intersting yes ok so this is very intersting becuse my log entries are also being comitted to the dev branch, which i guess is good if the dev entries are wack, but bad if i ever abandon a branch",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_11_15_22",
      "text": "Updating the entry protocol to hide the location tags and instead I have made a persistent stardate header with the location and date",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_11_19_56",
      "text": "updating the date format as well as the tag coding",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_12_23_03",
      "text": "back from unch and the stage.lib4ri.ch is back up",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_12_25_50",
      "text": "going to try to change y git config file in order to add come commands that can help me visualize the branches?",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_12_44_11",
      "text": "current calls that show pretty branches are git lg and lig wow. git wow is preferable. it's code is: git config --global alias.lg \"log --graph --abbrev-commit --decorate --all --color --date=relative --pretty=format:'%C(yellow)%h%Creset %C(cyan)(%ar)%Creset %C(magenta)<%an>%Creset %C(green)%d%Creset %s'\"",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_13_10_13",
      "text": "trying to get the rdm_nav on the website stage, but it seems like the permission structure is getting in the way, so now i need to got to the gitlab side again i think",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_13_10_33",
      "text": "merging branch FF_sugg into main in repo RDM_nav",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_05_24",
      "text": "DCC meeting virtual meeting, mostly from germany and the UK and canada",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_08_41",
      "text": "liise lehtsalu RDA europe assessing organisaitional context for RDSM POLICIES",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_12_17",
      "text": "PERSONAL DATA, IP AND OTHER INSTITUTIONAL AND CULTURAL REQUIRMENTS IS COUNTRY BY COUNTRY AND INSTITUTE BY INSTITUTE",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_16_31",
      "text": "scientific advancement is dependent not only on access to data, but also the software and hardware that is required to use, visualize and analyze those data. therefore RDM > RDSM",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_18_44",
      "text": "saftware is also a key research output, even if it is not yet aknowledged by universities",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_20_48",
      "text": "Research infrastructure self evaluration framework (RISE) 10 years old - ish, helps benchmark and evaluate where an org is and where it could be. covers 21 capabilities distributed over 10 areas",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_27_45",
      "text": "sometimes a stakeholder analysis internally is required in order to move forward with the RISE framework",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_28_49",
      "text": "Another framework is the ACME-FAIR Framework, where the focus is on enambling FAIR data. like RISE, ACME-FAIR offers an opportunity for discussion, and is done by a team",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_31_28",
      "text": "Liise Lehtsalu is a historian turned research support officer. Since 2015, she has been working in the Research Development Office in Eurac Research where her focus is on Open Science and Responsible Research and Innovation more broadly. She led the institutional open access policy development and currently focuses mainly on coordinating research data management and research ethics and integrity related efforts. Estonian-native, Lehtsalu landed in South Tyrol after years spent between Italy and the USA.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_33_18",
      "text": "ACME_FAIR is more complicated than RISE on account of its dual axis of maturity. moving away from service provision alone, and focuses on the user community",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_37_22",
      "text": "make it requires, make it rewarding, make it normanative, make it easy, make it possible",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_38_29",
      "text": "infrastructure > user interface/expereince>communities > incentives > Policy",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_14_54_29",
      "text": "doing activity with our group abour FAIR and ACMEFAIR",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_15_15_42",
      "text": "back in the main room and talking about areas of RDM that werent listed in the workbook",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_15_21_20",
      "text": "fair share, open aire, KPI resources, open research funders group for starting a data policy, no need to start froms cratch, all offer a checklist to structure the policy for a new institution.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_15_24_22",
      "text": "research software policies: Pro4RS resources, provides a template for a research software policy",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_15_27_44",
      "text": "FAIR enabling data policy checklist: self assess whether their policies are FAIR-enabling. what policy components are covered and in what degree of detail",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_15_33_17",
      "text": "the checklist includes a number of questions that focus on understanding what the policies shhould be enabling and what supports are in place to help with RDM",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_15_42_40",
      "text": "aim to be brief and concise, provide definitions, and avoid acronyms and jargon.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_16_02_06",
      "text": "now taking the time to read through oxfords data policy to determine some things we like and some things we don not like. https://researchdata.ox.ac.uk/sitefiles/university-of-oxford-research-data-management-policy.pdf",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_08_16_39_27",
      "text": "breakout rooms close, and we report on the likes and dislikes: many feel it was not specific enough, but it seems that policy is really not an implementation document. that falls under compliance and services",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_07_55_48",
      "text": "answering emails and pulling changes",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_08_11_48",
      "text": "answering emails and pulling changes",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_08_29_00",
      "text": "scheduling the Scientific IT services meeting in the library, making a subfolder for the DCC conference materials, fixing the 365 mailbox bug, and jumpiing back into the stage",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_08_46_16",
      "text": "working on tryin to host the web graphic somewhere else so that it can be used within an iframe on the website. Trying google cloud, but i think that switzerland will not like the use of google services, and also I think that I do not want to 'enable billing' even if it does turn out to be free.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_09_01_59",
      "text": "sent a message to frank and federico about hosting the tool on the web server to get around the troubles we are having with gitlab.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_09_06_19",
      "text": "creating a gitlab public version of the stardate app so i can version control my logs without everyine seeing them.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_09_48_13",
      "text": "stardate is now called 'logue' in the public facing part of the gitlab. I have uploaded the code as wellas the readme, the license, and the contributing documents at https://gitlab.eawag.ch/chase.nunez/logue",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_09_52_07",
      "text": "cleaning up deleted projects on gitlab. I have done this before, but somehow they keep coming back despite 'permenantly deleting' them",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_10_04_19",
      "text": "updating timesheet",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_10_19_42",
      "text": "committing chnages to the old log files, and setting the default editor to vim, also learning more about vim syntax",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_10_44_50",
      "text": "sending a reply email to susanne at PSI about our arrival tomorrow between 0900 and 0930",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_10_56_32",
      "text": "currently learning about modal editing, starting with the inline suggested tutorial: why to use nvim: https://youtu.be/TQn2hJeHQbM",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_11_12_36",
      "text": "nvim seems to be not worth it right now, but some basic vim does seem to help a bit. the editing functions seem to be robust and now that i know a bit more about the command and edit function, i think that making more complicated commits on git will be easier.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_12_29_18",
      "text": "lunch from 1140 - 1210",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_12_29_53",
      "text": "meeting with felix to make a page for the DMP as an example, meeting in C71 (if scheduling was done correctly)",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_12_54_20",
      "text": "I will format the example page on the stage, and Felix will draft some language to fill it",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_13_33_07",
      "text": "The website is now running on stage at: https://stage.lib4ri.ch/data-management-plans-eth-domain#accordian-item-title-",
      "tags": [
        "accordian",
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_14_24_04",
      "text": "updating the github page for the envidat projet ahead of our meeting with ionut",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_14_32_26",
      "text": "updating the stage website with example links",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_14_33_00",
      "text": "trying also to figure out why this application crashes if the window is too small",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_14_33_26",
      "text": "is it just width, or depth as well",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_14_43_56",
      "text": "It seems from the code that if there is not enough window real estate for both the current entry windoow and the entries fro today, when the window \"refreshes\" after an entry, it will crash.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_15_24_57",
      "text": "clearing out the nead_validator repo to focus on what is essentail for the NEAD validator, and getting rid of the past iterations that have led to NEAD_validator",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_15_47_52",
      "text": "adding a button feature to the Drupal lib4ri site",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_24_22",
      "text": "felix talked to florian about the grant, and he submitted on friday",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_27_44",
      "text": "m4 being lead by katharina, policy being constructed by some of the RI's, should be published in 26, consultation is ongoing.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_28_50",
      "text": "ASK IONUT about whether the consultation is ongoing with the policy document, EK is going to ask stuart or christian to see how the consultation is going at EAWAG",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_34_20",
      "text": "FF will share slides from the RDM conversation from Lothar about Scientific IT and the merger of repositories",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_35_15",
      "text": "if ANUBIS works for ERIC it can work for DORA",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_37_11",
      "text": "I need to reserve a room for IONUT",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_39_51",
      "text": "FF sets up a meeting with Stephie from EMPA for DMP process",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_43_14",
      "text": "questions about the capacity for Lib4RI to make customised front ends for the OpenBIS ELN",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_49_18",
      "text": "Chase asks Ionut: does wsl do a quality check for the DMP internally?",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_16_50_51",
      "text": "using the dmp income to stuart in order to target out services to starting research projects",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_17_18_46",
      "text": "in process: reproducable workflow section",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_17_20_38",
      "text": "in plan: reproducable searches, how to find, e.g. geodata?",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_09_17_32_39",
      "text": "meeting over, packing up and headed home",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_10_08_27_03",
      "text": "on the train to psi, left at 0810",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_10_08_33_01",
      "text": "revisiting the PSI programming document, perhaps its something FM and I can work on today",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_09_51_14",
      "text": "now at the PSI branch of Lib4RI: spent the morning getting our ID badges from the security office, then also getting our vpn set up from IT, now working in the library",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_10_02_36",
      "text": "organizing with EK re the ISCTI conference, the meeting with Federico re: server updates, and a jour fixe next week",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_11_00_22",
      "text": "sorting out the vpn in order to access the server. needs to be an external network for the vpn to work, excluding eduroam. so i access the guest wifi using mobile, then access the eawag wifi, then the server",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_11_08_00",
      "text": "ok now i will work on the wording for the 13 + new pages for the web page",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_11_09_29",
      "text": "first branching the rdm_nav",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_11_16_50",
      "text": "0810 + 0800 = 1610 + 0100  = 1710, means leaving PSI on on the 1606 bus",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_11_30_37",
      "text": "updating the Lib4RI_RDM_website_proposal to include the RDM nav",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_13_28_57",
      "text": "working with felix on the RDM website text, starting a repo at : https://gitlab.eawag.ch/chase.nunez/rdm_website_text.git",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_13_45_30",
      "text": "- discover - institutional support - consulting Services - Data Managment Plans - E-Notebooks - Data Storage each section will be a page with the following format: High level overview for the 4 RI's (75 words approx. ) RI specific summaries (40 words approx. for each of the following in this order) - EAWAG - EMPA - PSI - WSL",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_14_01_55",
      "text": "Discover",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_14_02_54",
      "text": "Discover Across the four ETH-domain research institutes, researchers have rich tools for finding literature and data. Lib4RI provides a unified search portal (Swisscovery/DORA) for publications and technical information [lib4ri.ch].Data discovery is supported by international registries (e.g. re3data) and domain portals. For example, the WSL EnviDat portal offers unified access to environmental datasets [wsl.ch], and subject-specific databases (e.g. Nautos for standards) are highlighted for Eawag, Empa, PSI and WSL users. Training and FAQs on advanced search are also offered through Lib4RI. Below, institute-level discovery support is summarized. Eawag Eawags library and RDM services provide access to publications via the Lib4RI DORA repository and catalog [dora.lib4ri.ch]. Research data can be found in Eawags open ERIC data repository (opendata.eawag.ch). Nautos (a shared Lib4RI database) centralizes standards and technical rules relevant to Eawag research. Users are advised on key databases in aquatic sciences and tools (e.g. literature databases, code archives) by library staff. Empa Empa researchers use the Lib4RI catalog and DORA for literature discovery, and are guided to relevant subject repositories (e.g. materials or engineering data). Empas ScientificIT team maintains a data portal (openBIS) that integrates lab instruments with FAIR archiving (publishing to Zenodo)[scientificit.empa.ch]. Like Eawag, Empa participates in Nautos for standards discovery. Empas data-management guidance and portal links point users to search tools and archives suited to their discipline. PSI PSI staff and users rely on Lib4RI services for literature search and are steered toward discipline repositories via registries. PSI runs the SciCat data catalog and SciLog electronic lab notebook (ELN) for facility data, and its formal Data Policy (2022) sets principles for data curation and three-year embargo rules. PSI provides metadata-rich indexes (SciCat) for data sets from its large facilities. In practice, users find publications via Lib4RI/DORA and data via PSIs own tools or international data portals. WSL WSLs literature search is served by Lib4RI (catalog and DORA) and by subject databases in forestry, snow and landscape science. For data, WSLs EnviDat portal is the central search interface  it publishes environmental datasets with DOIs and searchable metadata. Researchers can use EnviDats search across long-term monitoring and research data; for additional datasets they are directed to other relevant repositories (e.g. biodiversity or geodata). The library and EnviDat teams provide guidance on choosing and using these discovery tools. Institutional Support All four institutes maintain dedicated research-data management (RDM) support structures. These include RDM offices, IT teams or library liaisons that host policies, tools and guides for data stewardship. Each institute provides online RDM guidance, templates (e.g. SNF/Horizon-aligned templates) and consultations. Staff offer training and keep up-to-date with FAIR and funder requirements. For example, Eawags RDM service runs the ERIC repository and assists with DMPs and archiving [opendata.eawag.ch], while Empas digital science group oversees the openBIS platform and RDM best-practices [scientificit.empa.ch]. PSIs Data Curation group provides central SciCat/SciLog infrastructure and enforces its Data Policy. WSLs RDM is coordinated via the EnviDat team, linking day-to-day data curation with the institutional archive. In summary, each institute has an RDM program offering infrastructure (repositories, ELNs, storage) and advice to ensure compliance and FAIR data. Eawag Eawags Informatik and RDM services jointly run the Eawag Research Data Institutional Collection (ERIC). They maintain RDM documentation and an internal DMP template, and have staff available for data-management questions [opendata.eawag.ch]. The Informatik group provides server infrastructure and backup, and the Library offers standards and publication support. Together these institutional units ensure researchers have support for data organization, metadata, archiving and repository publication. Empa Empas Digital Science department provides a comprehensive RDM environment. It offers SNF-aligned DMP templates and best-practice guides (covering storage, backup, metadata, legal/ethical issues). Empa has implemented openBIS as a core platform: a shared ELN/LIMS system that streamlines data capture and FAIR archiving [scientificit.empa.ch]. Institutional support (ScientificIT and library staff) assists teams with DMP drafting, data handling and compliance. An intranet page centralizes guidance, and Empa maintains internal data storage with professional IT oversight. PSI PSIs RDM is driven by formal policy. The 2022 PSI Data Policy (binding for all users) defines ownership, curation, and access rules. A default 3year embargo applies to facility data (shorter or longer on request). Technically, PSI provides the SciCat data catalog and links to the campus tape archive. The Data Curation group offers tools and consultation for metadata standards and archives. Institutional support includes the research funding office and facility user offices that help users meet DMP and data publication requirements, ensuring alignment with PSI policy. WSL WSL coordinates data stewardship through the EnviDat program. EnviDats team develops the institutional portal (and data repository) while original data providers remain responsible for curation. WSL issues DMP templates and guidelines, connecting plan contents to appropriate repositories and metadata standards. The Library and EnviDat staff offer consulting: EnviDat manages DOI assignment and hosting of published datasets, while WSL researchers handle data preparation. This institutional framework ensures that long-term preservation and FAIR publishing are embedded in project workflows. Consulting Services Each institute offers expert consulting to help researchers with data and information needs. Lib4RIs digital desk and liaison librarians advise on literature and technical information; parallel RDM teams answer questions on data management, sharing and archiving. For example, Eawags RDM team (via rdm@eawag.ch) assists with FAIR data practices and repository selection. Empas Scientific IT support (scientificit@empa.ch) can guide on openBIS, DMPs and data workflows. PSIs Data Curation group consults on data cataloging, metadata and electronic logbooks (SciCat/SciLog). WSLs EnviDat service (envidat@wsl.ch) advises on publishing datasets and using the environmental data portal. In summary, personalized consultationsby email, workshops or one-on-one sessionsare available at each institute to support research projects from literature search through data sharing. Eawag Eawag researchers can contact library and RDM staff for tailored support. The RDM experts answer queries about DMPs, data repositories (ERIC and others), metadata standards, and publication workflows. Librarians and IT specialists assist with finding literature, databases, or technical information. This combined expertise ensures that users get hands-on advice for their specific data and publishing challenges. Empa Empa provides RDM consulting via its Scientific IT and library teams. Researchers can ask for help drafting SNF/Horizon DMPs, setting up the openBIS system, or following best-practice guidance (naming, backup, ethics). Support is available by email (scientificit@empa.ch) or through documentation on the Empa RDM portal [scientificit.empa.ch]. Empa also offers periodic training (e.g. on GitLab, FAIR data) and literature search consultations through Lib4RI services. PSI PSIs Science IT team (Data Curation group) offers consulting on facility data management. They assist with using SciCat/SciLog, with data policy compliance, and with finding suitable repositories. Data stewards can help structure user data, apply the PSI Data Policy rules, and prepare data for open access after embargo. The PSI Library collaborates on literature inquiries, while AWI user offices guide users through proposals, including DMP and facility obligations. WSL WSL consulting is led by the EnviDat team and library liaisons. Researchers get help publishing data via EnviDat and selecting other archives as needed. EnviDat staff advise on metadata and DOIs for datasets, and WSL RDM consultants link data-management plans to repository choices. The WSL Library assists with literature search and training, while EnviDat provides data-specific workshops. Contact the library or envidat@wsl.ch for personalized guidance on data sharing and archiving. Data Management Plans All four institutes emphasize funder-compliant, FAIR-aligned DMPs, with local support and tools. Each provides templates (typically SNF/Horizon formats) and guidance on content, plus review services. RDM teams and librarians work with researchers to ensure plans cover storage, metadata, ethical/legal issues and publication. In practice, institutes integrate DMP advice with existing infrastructure: Eawags RDM (which runs ERIC) explicitly links DMPs to its repository and workflows. Empa maintains guidance and an openBIS system to implement DMP commitments. PSIs binding Data Policy frames facility DMPs (e.g. data ownership, 3-year embargo). WSL connects DMPs to EnviDat publication and preservation. Below is a concise, institute-specific summary. Eawag Eawag uses an internal DMP template and guidance aligned with SNF/Horizon requirements. The RDM service (operating ERIC) ensures DMPs explicitly plan for archiving data in ERIC or other repositories. Eawags documentation provides examples of FAIR metadata and licensing. DMP review is offered by the Eawag RDM team (rdm@eawag.ch) to help integrate data-handling, storage and publication steps into proposals. Empa Empa offers an SNF-aligned DMP template on its RDM portal and detailed best-practice documentation. Researchers are advised to use openBIS for data organization and to deposit final data in repositories (e.g. Zenodo). The DMP guidance covers backup strategies, metadata schemes, legal aspects and data sharing plans. Empas Scientific IT staff (scientificit@empa.ch) and the Library can review draft plans and suggest resources to fulfill funder mandates. PSI PSIs DMP approach is anchored in policy. The PSI Data Policy (2022) is cited in proposals to guide data ownership and stewardship. DMPs must note the default 3year embargo and plan for eventual open access of facility data. PSI provides examples of discipline-appropriate archives and has dedicated sections in its DMP templates for facility data flows. Support is available from the Data Curation team and facility user offices to align DMPs with PSIs curation workflows. WSL WSL offers DMP templates and explicit guidance linking plan sections to data repositories and long-term archiving. The EnviDat portal team provides advice on choosing EnviDat (or other archives) based on dataset type. WSL emphasizes including file formats, metadata standards and preservation steps in the DMP. Librarians and EnviDat staff can review DMPs and suggest how EnviDats DOI assignment and curation services will fulfill repository commitments. ENotebooks Electronic lab notebooks (ELNs) help organize data at the source. ELN use varies by institute. Some offer dedicated platforms, while others rely on general tools. Common goals are automating metadata capture and linking data to workflows. FAIR principles apply equally. For example, Empa has deployed openBIS as its central ELN/LIMS, and PSI provides the SciLog ELN as part of SciCat. Training on using these systems is provided by IT groups. Below, we summarize ELN support per institute. Eawag Eawag currently does not provide an official ELN platform. Researchers may use general solutions (e.g. Git-based notebooks, MS OneNote, or discipline-specific tools) at their discretion. Interested users are encouraged to contact the Eawag RDM team (rdm@eawag.ch) for advice on suitable tools or developing institutional ELN solutions. The library also highlights best practices for electronic record-keeping. Empa Empas primary ELN/LIMS is the openBIS system, used by multiple labs for data acquisition and management. It combines ELN and inventory functions with FAIR data workflows. OpenBIS is integrated into lab processes at Empa (e.g. via QR codes and scripts) and supports archiving to Zenodo. Scientific IT provides openBIS training and support, and researchers are encouraged to use it for automated data logging, ensuring standardized metadata capture. PSI PSI offers SciLog, a centrally operated electronic logbook service, to PSI facility users. SciLog enables experiment recording and links with PSIs SciCat data catalog. PSI Data Curation staff advise on setting up experiments in SciLog and on best practices for electronic note-taking. The PSI library and SciCat team also support other ELN approaches (e.g. GitLab notebooks) through training and documentation. WSL WSL does not currently provide a dedicated institutional ELN platform. Researchers are free to use tools of their choice (e.g. lab-specific software or web-based notebooks). WSL recommends good data organization (e.g. using version control or structured file systems) and creating metadata to accompany experiment logs. Those interested in ELN solutions can consult the WSL RDM/ICT team (envidat@wsl.ch) for advice on emerging services and integration with EnviDat. Data Storage Secure and reliable data storage is provided at each institute, following best practices (e.g. the 3-2-1 backup rule). All institutes maintain on-premise file servers managed by their IT departments, with regular backups (often mirrored across sites). Notably, Eawag and Empa share a joint virtual server/storage cluster: data are synchronously replicated between the two Dbendorf data centers for high availability and disaster recovery. PSI provides high-capacity tape and disk archives (accessible via SciCat). For collaborative work and offsite access, ETH-operated cloud services are recommended (e.g. SWITCHdrive). All campuses encourage adherence to data protection rules (especially for sensitive data) and use of institute-managed storage over unsecured personal drives. See below for institute-specific storage provisions. Eawag Eawags research data are stored on a secure server infrastructure (shared with Empa) that mirrors critical data across two sites. This virtualized environment provides large capacity and continuous backup: if one data center fails, current data remain available in the other. On top of this, regular snapshot and tape backups protect against data loss. For file sharing, Eawag supports ETH SWITCH cloud storage services (SWITCHdrive) and advises teams to synchronize important data offsite. Empa Empa uses the same mirrored Eawag/Empa storage cluster, ensuring robust backup and uptime. Empas ICT department enforces routine backups (following the 3-2-1 rule) and maintains recovery procedures. In each location (Dbendorf, St. Gallen, Thun), project data are stored on network drives with periodic archiving to long-term tape. Empa researchers are encouraged to use department storage quotas and SWITCH cloud for flexibility, rather than local disks alone. PSI PSI maintains high-performance file servers for experimental data and integrates with a petabyte-scale archive system. The SciCat portal interfaces with this archive to ingest and preserve data. PSI IT applies regular backup routines to file systems, and users on campus have access to network home directories. For offsite collaboration, PSI provides SWITCHdrive (via ETH SWITCH) as part of its online tools. Compliance (e.g. data encryption) is enforced for sensitive or personal data. WSL WSLs ICT department operates centralized file servers (with RAID protection and backups) for all institute data. Researchers typically store data on these shared drives, which are backed up daily. Long-term archiving is done by transferring finalized datasets to EnviDat (for public data) or to offline media as needed. Cloud options (SwitchDrive, ETH Box, etc.) are available via ETH/SF network and supported through the WSL intranet. Overall, WSL recommends following industry best practices (321 rule) to safeguard research data.",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_14_29_35",
      "text": "there is a digital conference this afternoon from the university in amsterdam.",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_14_30_05",
      "text": "but I will let EK go instead an give us an update since we are working hard on the website text edits and expansion plan",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_14_30_49",
      "text": "I am also preparing the next generation of gitlab repo project photos using a free archive from an artists named logan voss",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_15_28_13",
      "text": "sending the policy workbook from the DCC workshop to a new location on the server: work/g_EResources/06_Projects/ORD_RDM/RDM_CDS_services/policy_development/resources",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_16_13_28",
      "text": "working on a photo reduction script call \"resize_to_200kb.sh\" for github icons",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_16_37_33",
      "text": "updating the text and read me of the website text repo",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_16_37_41",
      "text": "updating the repo images as well",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_16_54_39",
      "text": "messaging with Federico and Raol",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_10_16_54_50",
      "text": "updating recursive_shrink",
      "tags": [
        "psi"
      ],
      "location": "PSI"
    },
    {
      "timestamp": "2025_09_11_07_57_35",
      "text": "back at EAWAG, preparing for meeting with Ionut (WSL) this afternoon",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_08_26_47",
      "text": "Updating the sourcecode for stardate with a to do feature. stardate.json is now loaded as a data object that contains: entries  list of existing log entries (backwards compatible: if your file is still a list it will be migrated) tasks  mapping of YYYY_MM_DD  [task1, task2, ...] Added a second boxed input, Tasks for Tomorrow, where each submitted task is stored under tomorrow's date. Added display areas: Tasks for today: shows tasks that were created previously for today (i.e., tasks you added yesterday into the Tasks-for-Tomorrow box). Tasks for tomorrow (preview): shows tasks scheduled for tomorrow (so you can see what's coming). The main entry box remains Current entry: and boxed as before. Storage and git commit/push behavior unchanged (file still stardate.json, pushes performed in repo directory). Kept on-screen display concise  tags and location are still stored in JSON but not shown in the Today's Entries list. The code migrates older stardate.json that contains a bare list (old format) into the new dict format on first save, so nothing is lost.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_08_29_23",
      "text": "ok, well the UI froze a bit on last submission, and the earlier entries are no longer shown, so let's find out why that is. The entries are all a part of the same file, so it really is how the code is interpreting the json file",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_08_57_58",
      "text": "fixed a number of UI bugs and errors. the box is still not complete for some reason.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_08_58_20",
      "text": "however, the update gets rid of the github ghosting, which is a nice improvement",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_08_59_54",
      "text": "So now the user interface basically will ask every time for a task for tomorrow, or will allow you to press enter and add a task directly, but i am not so sure this is a good idea",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_09_35_44",
      "text": "ok now updating the program with new ways to implement tasks for tomorrow",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_09_55_41",
      "text": "updated and pushed a few chnages both to the aesthetics as wellas the tasking and entries code",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_09_56_10",
      "text": "",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_10_08_00",
      "text": "ok, now the UI and aesthetic changes have been completed (spacing between sections of the stardate interactive mode",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_10_14_55",
      "text": "Pushing final changes to the stardate UI coding",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_11_12_52",
      "text": "The program diles for stardate have now been overwritten as logue, for better or worse",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_11_13_04",
      "text": "",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_11_13_22",
      "text": "the logo also looks great",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_11_19_05",
      "text": "stardate is now officially depreciated, and going forward all logs will in logue",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_12_21_13",
      "text": "example entry",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_12_21_36",
      "text": "",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_13_29_45",
      "text": "working with Janina to reserve a room, but the permissions for my account do not permit acessing room calendars or reserving rooms.",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_14_39_25",
      "text": "meeting with ionut",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_14_43_18",
      "text": "will provide some real world examples , go to envidat 01 UI find ned files , maybe need to curate enrich csvs and create icsv",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    },
    {
      "timestamp": "2025_09_11_14_44_22",
      "text": "allow the users to edit the metadata and headers, andthen upload a new iCSV",
      "tags": [
        "eawag"
      ],
      "location": "EAWAG"
    }
  ],
  "tasks": {
    "2025_09_11": [
      "Ionut envidat_frictionless README.md changes",
      "stardate UI changes",
      "stardate add task subroutine",
      "stardate header changes"
    ],
    "2025_09_12": [
      "update copyright guidance with NASA suggestion",
      "update the production code for logue",
      "push changes to envidat_frictionless",
      "tasks seems to still work",
      "send emails"
    ]
  }
}